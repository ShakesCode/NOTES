===================================================
      HADOOP INSTALL CONFIG ON SINGLE NODE
===================================================
http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html
More detailed - https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.2/bk_installing_manually_book/content/ch_getting_ready_chapter.html
http://stackoverflow.com/questions/18862875/what-exactly-is-hadoop-namenode-formatting
Note: Also, try Cloudera Virtualbox sandbox VM option

Tutorials:
http://www.tutorialspoint.com/hadoop/hadoop_enviornment_setup.htm
http://had00b.blogspot.in/2013/08/setup-apache-hadoop-on-your-machine.html

MACHINE DETAILS
OS - Linux (Redhat/Oracle)
Install User - can be any user - does not require to be root or other specially set up user

DOWNLOAD LOCATION
http://hadoop.apache.org/releases.html
  http://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.7.2/hadoop-2.7.2.tar.gz
    http://mirror.fibergrid.in/apache/hadoop/common/hadoop-2.7.2/hadoop-2.7.2.tar.gz
    
INSTALL FOLDER
/disk1/opt/hadoop  (or /opt/hadoop)

DOWNLOAD
wget http://mirror.fibergrid.in/apache/hadoop/common/hadoop-2.7.2/hadoop-2.7.2.tar.gz

UNZIP
gunzip hadoop-2.7.2.tar.gz
tar xvfp gunzip hadoop-2.7.2.tar.gz

This installs Hadoop to folder /disk1/opt/hadoop/hadoop-2.7.2

INSTALL SSH AND RSYNC
http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html

-Install ssh
If it does not exist, (which ssh), install it

-Setup ssh to localhost
  $ ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
  $ cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys
  $ chmod 0600 ~/.ssh/authorized_keys
  
-Install rsync
# yum install rsync

-- - - - - - - 
REFERENCES FOR CONFIGURATION
-- - - - - - -
Refer: http://had00b.blogspot.in/2013/08/setup-apache-hadoop-on-your-machine.html
  echo "export HADOOP_INSTALL=~/hadoop-x.y.z" >> ~/.bashrc
  echo "export PATH=\$PATH:\$HADOOP_INSTALL/bin" >> ~/.bashrc
  source ~/.bashrc
  echo "export JAVA_HOME=$JAVA_HOME" >> $HADOOP_INSTALL/conf/hadoop-env.sh

Refer: http://www.tutorialspoint.com/hadoop/hadoop_enviornment_setup.htm
  export HADOOP_HOME=/usr/local/hadoop 
  export HADOOP_MAPRED_HOME=$HADOOP_HOME 
  export HADOOP_COMMON_HOME=$HADOOP_HOME 
  export HADOOP_HDFS_HOME=$HADOOP_HOME 
  export YARN_HOME=$HADOOP_HOME 
  export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native 
  export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin 
  export HADOOP_INSTALL=$HADOOP_HOME 

===================================
CONFIGURE - PSEUDO-DISTRIBUTED MODE
===================================
ENVIRONMENT
export JAVA_HOME=/usr/java/latest
export HADOOP_HOME=/disk1/opt/hadoop/hadoop-2.7.2
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME 
export HADOOP_COMMON_HOME=$HADOOP_HOME 
export HADOOP_HDFS_HOME=$HADOOP_HOME 
export YARN_HOME=$HADOOP_HOME 
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native 
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin 

DIRECTORIES
/disk1/data/hadoop/hdfs
/disk1/data/hadoop/hdfs/namenode
/disk1/data/hadoop/hdfs/datanode

CONFIGURATION FILE DESCRIPTIONS
Location -  HADOOP_HOME/etc/hadoop
Files- (description TBD)
capacity-scheduler.xml
core-site.xml --> contains information such as the port number used for Hadoop instance, 
                  memory allocated for the file system, memory limit for storing the data, and size of Read/Write buffers.
hadoop-policy.xml
hdfs-site.xml --> configure hdfs filesystems and replication count etc
httpfs-site.xml
kms-acls.xml
kms-site.xml
mapred-queues.xml.template
mapred-site.xml.template --> to specify the framework (like yarn) for mapreduce
ssl-client.xml.example
ssl-server.xml.example
yarn-site.xml --> to bring in yarn 
hadoop-env.sh --> environment variables setup for hadoop

HADOOP-ENV.SH
export JAVA_HOME=/usr/java/latest

CORE-SITE.XML
<configuration>
   <property>
      <name>fs.default.name </name>
      <value> hdfs://localhost:9000 </value> 
   </property>
</configuration>

HDFS-SITE.XML
<configuration>
   <property>
      <name>dfs.replication</name>
      <value>1</value>
   </property>
    
   <property>
      <name>dfs.name.dir</name>
      <value>file:///home/hadoop/hadoopinfra/hdfs/namenode </value>
   </property>
    
   <property>
      <name>dfs.data.dir</name> 
      <value>file:///home/hadoop/hadoopinfra/hdfs/datanode </value> 
   </property>
</configuration>

YARN-SITE.XML
<configuration>
  <property>
      <name>yarn.nodemanager.aux-services</name>
      <value>mapreduce_shuffle</value> 
   </property>
</configuration>

MAPRED-SITE.XML
<configuration>
   <property> 
      <name>mapreduce.framework.name</name>
      <value>yarn</value>
   </property>
</configuration>

FORMAT NAMENODE

$ hdfs namenode -format

NOTE: This creates the following directory 'namenode' and subdirectory 'current' and files:
(also, datanode directory is not created with this step)
/disk1/data/hadoop/hdfs/namenode/current: ls -l
total 16
-rw-r--r--. 1 dbgsm0 oraadm 353 Jun 24 00:01 fsimage_0000000000000000000
-rw-r--r--. 1 dbgsm0 oraadm  62 Jun 24 00:01 fsimage_0000000000000000000.md5
-rw-r--r--. 1 dbgsm0 oraadm   2 Jun 24 00:01 seen_txid
-rw-r--r--. 1 dbgsm0 oraadm 203 Jun 24 00:01 VERSION

On-screen messages:

16/06/24 00:01:02 INFO namenode.NameNode: STARTUP_MSG:
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = oralx0001.hq.target.com/10.63.166.33
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 2.7.2
STARTUP_MSG:   classpath = /disk1/opt/hadoop/hadoop-2.7.2/etc/hadoop:/disk1/opt/hadoop/hadoop-2.7.2/share/hadoop/common/lib/xmlenc-0.52.jar:... ... ... ...
16/06/24 00:01:02 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
16/06/24 00:01:02 INFO namenode.NameNode: createNameNode [-format]
16/06/24 00:01:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Formatting using clusterid: CID-9823b5b1-8701-475f-a6c5-414957175f3b
16/06/24 00:01:03 INFO namenode.FSNamesystem: No KeyProvider found.
16/06/24 00:01:03 INFO namenode.FSNamesystem: fsLock is fair:true
16/06/24 00:01:03 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
16/06/24 00:01:03 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
16/06/24 00:01:03 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
16/06/24 00:01:03 INFO blockmanagement.BlockManager: The block deletion will start around 2016 Jun 24 00:01:03
16/06/24 00:01:03 INFO util.GSet: Computing capacity for map BlocksMap
16/06/24 00:01:03 INFO util.GSet: VM type       = 64-bit
16/06/24 00:01:03 INFO util.GSet: 2.0% max memory 889 MB = 17.8 MB
16/06/24 00:01:03 INFO util.GSet: capacity      = 2^21 = 2097152 entries
16/06/24 00:01:03 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false
16/06/24 00:01:03 INFO blockmanagement.BlockManager: defaultReplication         = 1
16/06/24 00:01:03 INFO blockmanagement.BlockManager: maxReplication             = 512
16/06/24 00:01:03 INFO blockmanagement.BlockManager: minReplication             = 1
16/06/24 00:01:03 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2
16/06/24 00:01:03 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000
16/06/24 00:01:03 INFO blockmanagement.BlockManager: encryptDataTransfer        = false
16/06/24 00:01:03 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
16/06/24 00:01:03 INFO namenode.FSNamesystem: fsOwner             = <<username>> (auth:SIMPLE)
16/06/24 00:01:03 INFO namenode.FSNamesystem: supergroup          = supergroup
16/06/24 00:01:03 INFO namenode.FSNamesystem: isPermissionEnabled = true
16/06/24 00:01:03 INFO namenode.FSNamesystem: HA Enabled: false
16/06/24 00:01:03 INFO namenode.FSNamesystem: Append Enabled: true
16/06/24 00:01:04 INFO util.GSet: Computing capacity for map INodeMap
16/06/24 00:01:04 INFO util.GSet: VM type       = 64-bit
16/06/24 00:01:04 INFO util.GSet: 1.0% max memory 889 MB = 8.9 MB
16/06/24 00:01:04 INFO util.GSet: capacity      = 2^20 = 1048576 entries
16/06/24 00:01:04 INFO namenode.FSDirectory: ACLs enabled? false
16/06/24 00:01:04 INFO namenode.FSDirectory: XAttrs enabled? true
16/06/24 00:01:04 INFO namenode.FSDirectory: Maximum size of an xattr: 16384
16/06/24 00:01:04 INFO namenode.NameNode: Caching file names occuring more than 10 times
16/06/24 00:01:04 INFO util.GSet: Computing capacity for map cachedBlocks
16/06/24 00:01:04 INFO util.GSet: VM type       = 64-bit
16/06/24 00:01:04 INFO util.GSet: 0.25% max memory 889 MB = 2.2 MB
16/06/24 00:01:04 INFO util.GSet: capacity      = 2^18 = 262144 entries
16/06/24 00:01:04 INFO namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
16/06/24 00:01:04 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
16/06/24 00:01:04 INFO namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
16/06/24 00:01:04 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
16/06/24 00:01:04 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
16/06/24 00:01:04 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
16/06/24 00:01:04 INFO namenode.FSNamesystem: Retry cache on namenode is enabled
16/06/24 00:01:04 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
16/06/24 00:01:04 INFO util.GSet: Computing capacity for map NameNodeRetryCache
16/06/24 00:01:04 INFO util.GSet: VM type       = 64-bit
16/06/24 00:01:04 INFO util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
16/06/24 00:01:04 INFO util.GSet: capacity      = 2^15 = 32768 entries
16/06/24 00:01:04 INFO namenode.FSImage: Allocated new BlockPoolId: BP-167146619-<<IP Address>>-1466744464389
16/06/24 00:01:04 INFO common.Storage: Storage directory /disk1/data/hadoop/hdfs/namenode has been successfully formatted.
16/06/24 00:01:04 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
16/06/24 00:01:04 INFO util.ExitUtil: Exiting with status 0
16/06/24 00:01:04 INFO namenode.NameNode: SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at <<hostname>>/<<IP Address>>
************************************************************/

START DFS
$ start-dfs.sh
(NOTE - see the native library warning at the end - this needs fix - see fixes after these messages below)

16/06/24 00:18:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Incorrect configuration: namenode address dfs.namenode.servicerpc-address or dfs.namenode.rpc-address is not configured.
Starting namenodes on []
localhost: *************************************************************************
localhost: << banner messages due to ssh >>
localhost: *************************************************************************
localhost: starting namenode, logging to /disk1/opt/hadoop/hadoop-2.7.2/logs/hadoop-<username>-namenode-<hostname>.out
localhost: *************************************************************************
localhost: << banner messages due to ssh >>
localhost: *************************************************************************
localhost: starting datanode, logging to /disk1/opt/hadoop/hadoop-2.7.2/logs/hadoop-<username>-namenode-<hostname>.out
Starting secondary namenodes [0.0.0.0]
0.0.0.0: *************************************************************************
localhost: << banner messages due to ssh >>
0.0.0.0: *************************************************************************
0.0.0.0: starting secondarynamenode, logging to /disk1/opt/hadoop/hadoop-2.7.2/logs/hadoop-<username>-namenode-<hostname>.out
16/06/24 00:19:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable

FIX FOR NATIVE LIBRARY WARNING
http://stackoverflow.com/questions/19943766/hadoop-unable-to-load-native-hadoop-library-for-your-platform-warning

- Hadoop native libraries are in HADOOP_HOME/lib/native (/disk1/opt/hadoop/hadoop-2.7.2/lib/native)
- Check if the library is 64 bit:
$ ldd libhadoop.so.1.0.0
./libhadoop.so.1.0.0: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by ./libhadoop.so.1.0.0)
linux-vdso.so.1 =>  (0x00007fff43510000)
libdl.so.2 => /lib64/libdl.so.2 (0x00007f9be553a000)
libc.so.6 => /lib64/libc.so.6 (0x00007f9be51a5000)
/lib64/ld-linux-x86-64.so.2 (0x00007f9be5966000)

- Modify HADOOP_OPTS as follows in hadoop-env.sh (and also can include in .profile)
export HADOOP_OPTS="-XX:-PrintWarnings -Djava.net.preferIPv4Stack=true -Djava.library.path=/disk1/opt/hadoop/hadoop-2.7.2/lib/native"

- More errors:
sed: -e expression #1, char 6: unknown option to `s'
-c: Unknown cipher type 'cd'
....
....
Using: ssh: Could not resolve hostname using: Name or service not known
-: ssh: Could not resolve hostname -: Name or service not known
INFO: ssh: Could not resolve hostname info: Name or service not known
from: ssh: Could not resolve hostname from: Name or service not known

===>> This may require  building hadoop from source as the o/s is 64 bit: http://cleverowl.uk/2015/06/30/compiling-2-7-0-on-64-bit-linux/

========================================
BUILD HADOOP FROM SOURCE FOR 64BIT LINUX
========================================
http://cleverowl.uk/2015/06/30/compiling-2-7-0-on-64-bit-linux/

DOWNLOAD 64BIT JDK
http://download.oracle.com/otn-pub/java/jdk/8u91-b14/jdk-8u91-linux-x64.tar.gz
wget --no-check-certificate --no-cookies --header "Cookie: oraclelicense=accept-securebackup-cookie" http://download.oracle.com/otn-pub/java/jdk/8u91-b14/jdk-8u91-linux-x64.tar.gz

EXTRACT JDK
Extract the downloaded file into /disk1/opt/java/64/
say, /disk1/opt/java/64/jdk1.8.0_91



