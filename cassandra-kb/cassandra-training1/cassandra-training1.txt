https://dzone.com/articles/introduction-apache-cassandras

RDBMS
Strict schema
Not natively distributed - distributed is an additional feature
- so, not natively scalable
ACID

NO-SQL
Schema free - dynamic schema
Distributed natively - that is the feature
CAP instead of ACID
- Consistency, Availability, Parititioning
- Consistency is inversely proportional to partitioning
- Performance higher with more partitions

NO SQL CATEGORIES
key-value - redis
document oriented (is actually a row) - mongo, couch, dynamo…
column oriented - key-value - cassandra, hbase 
graph oriented

Mongo - heavy read ok, heavy write not ok
Cassandra - heavy write ok, heavy reads not that ok
Cassandra - does not have a single master node - so more failure tolerant

HBase is Hadoop ‘database’ - like a database designed for Hadoop workloads

CASSANDRA
Multiple machines
Write anywhere - yet user sees the whole thing as a unified one
In mysql, bigtable etc some nodes should be set up as slaves
—> but, cassandra it is decentralized - even the node/master/replication management is distributed
—> peer-to-peer ‘gossip’ protocol to keep in synch with other nodes
—> so, NO SINGLE POINT OF FAILURE - high availability ensured
Nodes can be added and removed dynamically without downtime 
- seamless scale up/down
- no downtime for the cluster overall
Patching within same major versions can be upgraded in rolling fashion

CLUSTER
Data-Centers - DC1,DC2... - for disaster recovery
Each DC with multiple racks and each rack with multiple nodes
Nodes can have different minor-versions within the same major version (like 3.1, 3.2 etc)

CQL - Cassandra - JDK - OS
- CQL is the high level API to access Cassandra
- Cassandra is written in Java
- It will talk to OS via JDK

NODE, DATA CENTER, CLUSTER
https://www.guru99.com/cassandra-architecture.html
Node
Node is the place where data is stored. It is the basic component of Cassandra.

Data Center
A collection of nodes are called data center. Many nodes are categorized as a data center.

Cluster
The cluster is the collection of many data centers.

Commit Log
Every write operation is written to Commit Log. Commit log is used for crash recovery.

Mem-table
After data written in Commit log, data is written in Mem-table. Data is written in Mem-table temporarily.

SSTable
When Mem-table reaches a certain threshold, data is flushed to an SSTable disk file.

TOKENS 
It is the 'vnodes' Cassandra creates within the node - to distribute data within the node.
The number 256 is default which is an optimal benchmkark for 8GB 2CPU Cassandra node.
Higher number of tokens per node can be given for higher capacity machines.

Depending upon number of total machines and their capacities define a total number of tokens 
and then divide them across nodes.

Murmur algorithm does the replica allocation across nodes - no need to manually set the token limit values

REPLICATION
Replication is for a keyspace - not a table or a node

WRITES HOW IT WORKS
First writes go to memory - as a 'memtable'
After the memory threshold is reached - it gets flushed to disk
Then it writes to commit logs and then to table 'sstable'
sstable compacts periodically (can be set) to compact fragmentation

If memtable threshold is increased much - then if node fails then that much data will be lost and needs recovery

COMPACTION
Compaction-Strategy
Different strategies for different types of data like time-series data etc
Tombstone-deletes
Threshold minimum 4

READS
First looks in memory (like in Oracle) 
Checks whether the node owns the data or not (in key-cache) - then checks in memory (row-cache)
Gets, misses
LRU row-cache

MONITORING
tpstatus

=========================
MULTINODE CLUSTER
=========================
NOTE: For steps, see CREATE CLUSTER section
/etc/cassandra/cassandra.yaml
Bootstrapping

Change these your actual IP Address:
seedIP
ListenAddress
RPC_Address

Autobootstrap = off means it starts as a single node, but ready for bootstrapping into a cluster

Snitch:
Discover topology of the network (like racks, data centers etc)

============
PARTITIONER
============
Murmur3 partition hashes everything into a key of 64bit integer - which will be based on the PK of the record itself.
Then it will distribute it to nodes based on the tokens allocated of each node and some token-range algorithm.
To find where a row actually sits:  nodetool ring <keyspace name>

$ nodetool ring ks0 | wc
1546   12324  189349

NOTE:  The 1546 above is the total tokens/vnodes - 256 tokens per node multiplied by 6 nodes

$ nodetool ring ks0 |head
Datacenter: dc1
==========
Address        Rack        Status State   Load            Owns                Token                                       
                                                                              9222365415242980500                         
172.31.85.72   rack1       Up     Normal  230.18 KiB      16.40%              -9210043340703805705                        
172.31.95.127  rack1       Up     Normal  70.8 KiB        16.38%              -9204249525111820733                        
172.31.85.72   rack1       Up     Normal  230.18 KiB      16.40%              -9201888592744065084                        
172.31.82.133  rack1       Up     Normal  120.17 KiB      16.57%              -9188869267995685337                        
172.31.95.127  rack1       Up     Normal  70.8 KiB        16.38%              -9173690029360339008                        
172.31.84.180  rack1       Up     Normal  355.95 KiB      17.48%              -9145122507948764650  

=========================
DATA CENTERS
=========================

Data Center1 - rack1, rack2
	rack1 - node1a, node1b, node1c, node1d
	rack2 - node1a, node1b, node1c, node1d

Data Center2 - rack1, rack2
	rack1 - node1a, node1b, node1c, node1d
	rack2 - node1a, node1b, node1c, node1d
	
Network topology is understood by 'snitch'

Default configuration dc1,rack1 - one data center, one rack

Cassandra tries to store data across data centers for HADR - partition tolerance
- and tries to route queries to local data centers

--------------
GOSSIP
--------------
Gossip protocol - failure detection - allows each node to keep track of state info about other nodes in the cluster
- runs every second on a timer
- just keeps track of metadata of node-alive - and does not bother about which data is stored where

Seed node knows everything about all nodes
Gossip looks for info in 3 neighboring nodes - and those nodes connect with three other
Gossip verifies for up/down kind of info

Every second a node returns GossipDigestAckMessage and then a return-message is received
- Phi Accrual Failure Detection
- Nodes not responding compared to previously acknowledging-nodes list are considered down

Threshold is possible (like 3 gossips) before declaring a node is down

--------------
SNITCH
--------------
Snitches gather info about network topology
- figure out where nodes are in relation to other nodes

Different snitches for Amazon EC2, Google Cloud, Apache Cloudstack etc

Package org.apache.cassandra.locator
- IEndpointSnitch interface

GossipingPropertyFileSnitch, SimpleSnitch

Dynamic Snitching
- Cassandra will be able to figure out which nodes are how 'far' or same/different network - based on response speeds

--- SOME POINTS TBD
Coordinator nodes
ring vs cluster vs datacenter
In connect string provide local nodes first remote nodes later
Do cross-data center replication
--------

RING - total collection of data in a cluster - which contradicts a bit - that within a datacenter also it is a ring
Token - hash number created for a PK
Virtual Nodes - 256 tokens by default (tokens count = vnode count)
num_token property
More tokens for bigger machines

PARTITIONER - default is murmur3
SimpleStrategy, NetworkTopologyStrategy
NetworkTopologyStrategy - looks for differentdata centers and racks to provide HA-DR replication

-------------------
CONSISTENCY LEVELS
-------------------
Terminologies - consistency level, eventual consistency, tunably consistent system

Levels - one, two, three - absolute numer of replica nodes that must respond to a request
- applies to read and write (set separately)
The QUORUM consistency level requires response from a majority of the replica nodes (relication factor / 2) + 1 nodes

Write - default consistency is one - so it writes to one node only - and wait for only one node to respond
- fails if that node is down
- (and replication happens in the background)
- have higher consistency for writes so that write to at least one node is successful

Read - default consistency is one - so, if the node is down, reads fail
- if level is higher than one, there could be data mismatch

Replication factor = 1 and consistency > 1 is not possible

Cases: Assume Replication-Factor = 3
wc=1, rc=1 - reads will be fastest as read is from one node, but may return inconsistent data
wc=1, rc=2 - reads will wait for two replicas to give response - what if it reads from two stale replicas?
- Java client drivers are intelligent to get the better consistent one 
-- that intelligence is called 'READ REPAIR' - which accepts the later timestamp and sends message to stale replica
-- THIS NEEDS THE MULTIPLE DATA CENTERS BE IN SAME TIME ZONE or UTC/GMT

* STRONG CONSISTENCY - wc=1, rc=3 - This will be CONSISTENT as all 3 replicas respond and client reads latest from them
- FAST WRITE, SLOW READ

wc=2, rc=2 - does not give much consistency

* STRONG CONSISTENCY - wc=2, rc=2 - with replication factor=3 gives very good consistency
- READ AND WRITE WELL TUNED FOR SPEED AND CONSISTENCY

* STRONG CONSISTENCY - wc=3, rc=1 - strong consitency
- READ FAST, WRITES A BIT SLOW


Questions:
1. If can write to any, but data will sit some place in the ring based on token of the key
- Right

2. Does cassandra have a concept of virtual/scan-ip for the whole cluster, 
   which can avoid us giving multiple-IPs as contact points 
- No, that has to be done by an extenral load-balancer or Fastly and such

3. In reads - instead of giving contact points of local data center - can we give a virtual IP for the whole cluster
   and the cluster can figure out where the query is coming from and direct to local datacenter
- No, that has to be done by an extenral load-balancer or Fastly and such


== ********* ==
====================================================================================================
=========================
TECHNICAL THINGS
=========================
COMMANDS, FILES ETC

/var/log/cassandra
/var/lib/cassandra
/var/lib/cassandra/data
/etc/cassandra/cassandra.yaml

nodetool status
cqlsh

=========================
INSTALL CONFIG on Ubuntu
=========================
# echo "deb http://www.apache.org/dist/cassandra/debian 311x main" | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list

# curl https://www.apache.org/dist/cassandra/KEYS | sudo apt-key add -

# apt-get update
# apt-get install cassandra  -y

# which cassandra
/usr/sbin/cassandra

# which cqlsh
/usr/bin/cqlsh

# service  cassandra status
 * Cassandra is running
 
# ls /var/lib/cassandra/
commitlog  data  hints  saved_caches

# ls /var/lib/cassandra/data/
system  system_auth  system_distributed  system_schema  system_traces

# tail /var/log/cassandra/system.log

# nodetool status
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load       Tokens       Owns (effective)  Host ID                               Rack
UN  127.0.0.1  103.67 KiB  256          100.0%            5975effd-cd85-4f8e-a83c-c66bf7840689  rack1

$ cqlsh
Connected to Test Cluster at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 3.11.2 | CQL spec 3.4.4 | Native protocol v4]
Use HELP for help.
cqlsh> describe keyspaces

system_traces  system_schema  system_auth  system  system_distributed

cqlsh> create keyspace ks0 with replication = {'class':'SimpleStrategy','replication_factor':1};
cqlsh> describe keyspace ks0

CREATE KEYSPACE ks0 WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'}  AND durable_writes = true;

cqlsh> use ks0;
cqlsh:ks0> 

cqlsh:ks0> CREATE TABLE users(user_id varchar, age int, email varchar, city varchar, PRIMARY KEY (user_id));
cqlsh:ks0> describe ks0.users

CREATE TABLE ks0.users (
    user_id text PRIMARY KEY,
    age int,
    city text,
    email text
) WITH bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';
    
cqlsh:ks0> INSERT INTO users(user_id, age, email, city) VALUES ('jsmith',32,'john.smith@example.com','Dallas');
cqlsh:ks0> 
cqlsh:ks0> select * from users;

 user_id | age | city   | email
---------+-----+--------+------------------------
  jsmith |  32 | Dallas | john.smith@example.com

(1 rows)
cqlsh:ks0> 

$ nodetool status
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load       Tokens       Owns (effective)  Host ID                               Rack
UN  127.0.0.1  200.17 KiB  256          100.0%            5975effd-cd85-4f8e-a83c-c66bf7840689  rack1

$ nodetool status admatic24

Error: The keyspace admatic24, does not exist

$ nodetool status ks0
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load       Tokens       Owns (effective)  Host ID                               Rack
UN  127.0.0.1  200.17 KiB  256          100.0%            5975effd-cd85-4f8e-a83c-c66bf7840689  rack1

$ nodetool info
ID                     : 5975effd-cd85-4f8e-a83c-c66bf7840689
Gossip active          : true
Thrift active          : false
Native Transport active: true
Load                   : 200.17 KiB
Generation No          : 1530523374
Uptime (seconds)       : 83348
Heap Memory (MB)       : 224.70 / 990.00
Off Heap Memory (MB)   : 0.00
Data Center            : datacenter1
Rack                   : rack1
Exceptions             : 0
Key Cache              : entries 27, size 2.34 KiB, capacity 49 MiB, 130 hits, 168 requests, 0.774 recent hit rate, 14400 save period in seconds
Row Cache              : entries 0, size 0 bytes, capacity 0 bytes, 0 hits, 0 requests, NaN recent hit rate, 0 save period in seconds
Counter Cache          : entries 0, size 0 bytes, capacity 24 MiB, 0 hits, 0 requests, NaN recent hit rate, 7200 save period in seconds
Chunk Cache            : entries 20, size 1.25 MiB, capacity 215 MiB, 222 misses, 542 requests, 0.590 recent hit rate, NaN microseconds miss latency
Percent Repaired       : 100.0%
Token                  : (invoke with -T/--tokens to see all 256 tokens)

$ nodetool cfstats ks0
Total number of tables: 37
----------------
Keyspace : ks0
	Read Count: 0
	Read Latency: NaN ms
	Write Count: 8
	Write Latency: 0.09575 ms
	Pending Flushes: 0
		Table: users
		SSTable count: 1
		Space used (live): 5237
		Space used (total): 5237
		Space used by snapshots (total): 0
		Off heap memory used (total): 51
		SSTable Compression Ratio: 0.31970260223048325
		Number of partitions (estimate): 8
		Memtable cell count: 0
		Memtable data size: 0
		Memtable off heap memory used: 0
		Memtable switch count: 4
		Local read count: 0
		Local read latency: NaN ms
		Local write count: 8
		Local write latency: NaN ms
		Pending flushes: 0
		Percent repaired: 0.0
		Bloom filter false positives: 0
		Bloom filter false ratio: 0.00000
		Bloom filter space used: 24
		Bloom filter off heap memory used: 16
		Index summary off heap memory used: 19
		Compression metadata off heap memory used: 16
		Compacted partition minimum bytes: 61
		Compacted partition maximum bytes: 72
		Compacted partition mean bytes: 72
		Average live cells per slice (last five minutes): NaN
		Maximum live cells per slice (last five minutes): 0
		Average tombstones per slice (last five minutes): NaN
		Maximum tombstones per slice (last five minutes): 0
		Dropped Mutations: 0

----------------

DO SOME INSERTS
cqlsh:ks0> INSERT INTO users(user_id, age, email, city) VALUES ('jsmith9',32,'john.smith@example.com','Dallas');
cqlsh:ks0> INSERT INTO users(user_id, age, email, city) VALUES ('jsmith10',32,'john.smith@example.com','Dallas');
cqlsh:ks0> INSERT INTO users(user_id, age, email, city) VALUES ('jsmith11',32,'john.smith@example.com','Dallas');

CHECK MEMTABLE CHANGES
$ nodetool cfstats ks0
Total number of tables: 37
----------------
Keyspace : ks0
	Read Count: 0
	Read Latency: NaN ms
	Write Count: 11
	Write Latency: 0.08963636363636364 ms
	Pending Flushes: 0
		Table: users
		SSTable count: 1
		Space used (live): 5237
		Space used (total): 5237
		Space used by snapshots (total): 0
		Off heap memory used (total): 51
		SSTable Compression Ratio: 0.31970260223048325
		Number of partitions (estimate): 11
		Memtable cell count: 3
		Memtable data size: 327
		Memtable off heap memory used: 0
		Memtable switch count: 4
		Local read count: 0
		Local read latency: NaN ms
		Local write count: 11
		Local write latency: 0.085 ms
		Pending flushes: 0
		Percent repaired: 0.0
		Bloom filter false positives: 0
		Bloom filter false ratio: 0.00000
		Bloom filter space used: 24
		Bloom filter off heap memory used: 16
		Index summary off heap memory used: 19
		Compression metadata off heap memory used: 16
		Compacted partition minimum bytes: 61
		Compacted partition maximum bytes: 72
		Compacted partition mean bytes: 72
		Average live cells per slice (last five minutes): NaN
		Maximum live cells per slice (last five minutes): 0
		Average tombstones per slice (last five minutes): NaN
		Maximum tombstones per slice (last five minutes): 0
		Dropped Mutations: 0

----------------

FLUSH
$ nodetool flush

CHECK MEMTABLE CHANGES
$ nodetool cfstats ks0
Total number of tables: 37
----------------
Keyspace : ks0
	Read Count: 0
	Read Latency: NaN ms
	Write Count: 11
	Write Latency: 0.08963636363636364 ms
	Pending Flushes: 0
		Table: users
		SSTable count: 2
		Space used (live): 10313
		Space used (total): 10313
		Space used by snapshots (total): 0
		Off heap memory used (total): 86
		SSTable Compression Ratio: 0.3795918367346939
		Number of partitions (estimate): 11
		Memtable cell count: 0
		Memtable data size: 0
		Memtable off heap memory used: 0
		Memtable switch count: 5
		Local read count: 0
		Local read latency: NaN ms
		Local write count: 11
		Local write latency: NaN ms
		Pending flushes: 0
		Percent repaired: 0.0
		Bloom filter false positives: 0
		Bloom filter false ratio: 0.00000
		Bloom filter space used: 40
		Bloom filter off heap memory used: 24
		Index summary off heap memory used: 38
		Compression metadata off heap memory used: 24
		Compacted partition minimum bytes: 61
		Compacted partition maximum bytes: 72
		Compacted partition mean bytes: 72
		Average live cells per slice (last five minutes): NaN
		Maximum live cells per slice (last five minutes): 0
		Average tombstones per slice (last five minutes): NaN
		Maximum tombstones per slice (last five minutes): 0
		Dropped Mutations: 0

----------------

$ cqlsh
Connected to Test Cluster at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 3.11.2 | CQL spec 3.4.4 | Native protocol v4]
Use HELP for help.

cqlsh> use ks0 ;
cqlsh:ks0> INSERT INTO users(user_id, age, email, city) VALUES ('jsmith9',32,'john.smith@example.com','Dallas');
cqlsh:ks0> INSERT INTO users(user_id, age, email, city) VALUES ('jsmith10',32,'john.smith@example.com','Dallas');
cqlsh:ks0> INSERT INTO users(user_id, age, email, city) VALUES ('jsmith11',32,'john.smith@example.com','Dallas');

$ nodetool cfstats ks0
Total number of tables: 46
----------------
Keyspace : ks0
	Read Count: 0
	Read Latency: NaN ms
	Write Count: 13
	Write Latency: 0.08592307692307692 ms
	Pending Flushes: 0
		Table: emp
		SSTable count: 2
		Space used (live): 10499
		Space used (total): 10499
		Space used by snapshots (total): 0
		Off heap memory used (total): 80
		SSTable Compression Ratio: 0.5707692307692308
		Number of partitions (estimate): 10
		Memtable cell count: 1
		Memtable data size: 112
		Memtable off heap memory used: 0
		Memtable switch count: 4
		Local read count: 0
		Local read latency: NaN ms
		Local write count: 13
		Local write latency: 0.219 ms
		Pending flushes: 0
		Percent repaired: 0.0
		Bloom filter false positives: 0
		Bloom filter false ratio: 0.00000
		Bloom filter space used: 40
		Bloom filter off heap memory used: 24
		Index summary off heap memory used: 32
		Compression metadata off heap memory used: 24
		Compacted partition minimum bytes: 51
		Compacted partition maximum bytes: 60
		Compacted partition mean bytes: 60
		Average live cells per slice (last five minutes): NaN
		Maximum live cells per slice (last five minutes): 0
		Average tombstones per slice (last five minutes): NaN
		Maximum tombstones per slice (last five minutes): 0
		Dropped Mutations: 0

----------------
$ nodetool compactionstats
pending tasks: 0

$
$ ls /var/lib/cassandra/data/ks0/users-c0d79fb05d7a11e8a4c9d52e94b3f8eb/
backups                      mc-4-big-Index.db            mc-5-big-Data.db        mc-5-big-Summary.db
mc-4-big-CompressionInfo.db  mc-4-big-Statistics.db       mc-5-big-Digest.crc32   mc-5-big-TOC.txt
mc-4-big-Data.db             mc-4-big-Summary.db          mc-5-big-Filter.db
mc-4-big-Digest.crc32        mc-4-big-TOC.txt             mc-5-big-Index.db
mc-4-big-Filter.db           mc-5-big-CompressionInfo.db  mc-5-big-Statistics.db

$ nodetool compactionstats
pending tasks: 0

$ sudo nodetool compact

$ sudo nodetool compact
$ nodetool compactionstats
pending tasks: 0

$ nodetool gcstats
       Interval (ms) Max GC Elapsed (ms)Total GC Elapsed (ms)Stdev GC Elapsed (ms)   GC Reclaimed (MB)         Collections      Direct Memory Bytes
            80388500                 199                1557                  22          5920033952                  72                       -1

$ nodetool tpstats
Pool Name                         Active   Pending      Completed   Blocked  All time blocked
ReadStage                              0         0            376         0                 0
MiscStage                              0         0              0         0                 0
CompactionExecutor                     0         0          50080         0                 0
MutationStage                          0         0             20         0                 0
MemtableReclaimMemory                  0         0            134         0                 0
PendingRangeCalculator                 0         0              1         0                 0
GossipStage                            0         0              0         0                 0
SecondaryIndexManagement               0         0              0         0                 0
HintsDispatcher                        0         0              0         0                 0
RequestResponseStage                   0         0              0         0                 0
Native-Transport-Requests              0         0           2116         0                 0
ReadRepairStage                        0         0              0         0                 0
CounterMutationStage                   0         0              0         0                 0
MigrationStage                         0         0             21         0                 0
MemtablePostFlush                      0         0            379         0                 0
PerDiskMemtableFlushWriter_0           0         0            134         0                 0
ValidationExecutor                     0         0              0         0                 0
Sampler                                0         0              0         0                 0
MemtableFlushWriter                    0         0            134         0                 0
InternalResponseStage                  0         0              0         0                 0
ViewMutationStage                      0         0              0         0                 0
AntiEntropyStage                       0         0              0         0                 0
CacheCleanupExecutor                   0         0              0         0                 0

Message type           Dropped
READ                         0
RANGE_SLICE                  0
_TRACE                       0
HINT                         0
MUTATION                     0
COUNTER_MUTATION             0
BATCH_STORE                  0
BATCH_REMOVE                 0
REQUEST_RESPONSE             0
PAGED_RANGE                  0
READ_REPAIR                  0
$

=======================
CREATE CLUSTER
=======================

--------------------------------
CREATE WITH ONLY ONE SEED
--------------------------------

- Configure
On each node, edit /etc/cassandra/cassandra.yml.
Give the private IP of the seed node against seeds.
Give the private IP of the local node against listen_address and rpc_address.

-- Settings to be done in cassandra.yml

# diff cassandra.yaml.orig cassandra.yaml
10c10
< cluster_name: 'Test Cluster'
---
> cluster_name: 'Team4Cluster'
425c425
<           - seeds: "127.0.0.1"
---
>           - seeds: "172.31.84.180"
599c599
< listen_address: localhost
---
> listen_address: 172.31.85.72
676c676
< rpc_address: localhost
---
> rpc_address: 172.31.85.72
949c949
< endpoint_snitch: SimpleSnitch
---
> endpoint_snitch: GossipingPropertyFileSnitch
1237a1238,1239
> #
> auto_bootstrap: false

- Remove the folder /var/lib/cassandra/data

- Verify

$ nodetool status
Datacenter: dc1
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address        Load       Tokens       Owns    Host ID                               Rack
UN  172.31.95.127  70.8 KiB   256          ?       66002181-ba36-4cfe-9b3a-d165139440ff  rack1
UN  172.31.92.87   155.1 KiB  256          ?       8f2f9d78-a33c-47f9-90b5-811d9670d410  rack1
UN  172.31.82.133  120.17 KiB  256          ?       da0d2899-c2c2-4bb3-8267-a590c0ea12ff  rack1
UN  172.31.84.180  355.95 KiB  256          ?       3b6e8368-9274-4238-8a6c-3ff1fb7bd4b1  rack1

Note: Non-system keyspaces don't have the same replication settings, effective ownership information is meaningless

# nodetool status godisgreat
Datacenter: dc1
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address        Load       Tokens       Owns (effective)  Host ID                               Rack
UN  172.31.85.72   170.09 KiB  256          25.2%             b23352ce-bd97-4713-983a-a0715620a4d7  rack1
UN  172.31.92.87   135.23 KiB  256          24.8%             8f2f9d78-a33c-47f9-90b5-811d9670d410  rack1
UN  172.31.82.133  142.94 KiB  256          24.1%             da0d2899-c2c2-4bb3-8267-a590c0ea12ff  rack1
UN  172.31.84.180  143.17 KiB  256          25.8%             3b6e8368-9274-4238-8a6c-3ff1fb7bd4b1  rack1

- Bring down one node and see status
$ nodetool status godisgreat
Datacenter: dc1
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address        Load       Tokens       Owns (effective)  Host ID                               Rack
UN  172.31.95.127  70.8 KiB   256          16.4%             66002181-ba36-4cfe-9b3a-d165139440ff  rack1
UN  172.31.90.156  193.64 KiB  256          16.5%             6570d39b-3af4-4d5a-9c21-ab694f563f4b  rack1
UN  172.31.85.72   230.18 KiB  256          16.4%             b23352ce-bd97-4713-983a-a0715620a4d7  rack1
UN  172.31.92.87   155.1 KiB  256          16.6%             8f2f9d78-a33c-47f9-90b5-811d9670d410  rack1
UN  172.31.82.133  120.17 KiB  256          16.6%             da0d2899-c2c2-4bb3-8267-a590c0ea12ff  rack1
DN  172.31.84.180  243.7 KiB  256          17.5%             3b6e8368-9274-4238-8a6c-3ff1fb7bd4b1  rack1

- Connect and run a create table
Note: You now need to give IP address as loopback will not work anymore
$ cqlsh 172.31.85.72

- IF THE SEED SERVER(s) ITSELF IS DOWN
You will get NoHostAvailable error even though other machines are up.

cqlsh:godisgreat> select * from users;
NoHostAvailable: 

- To know which row is in which token (combine this with nodetool status ring output to know on which node)
cqlsh:godisgreat> select token(user_id), user_id from users;

 system.token(user_id) | user_id
-----------------------+----------
  -7517536897081353922 |     kris
   -400498980410088294 |     UID2
   -280105036964296635 | coder1
   3387803449176249109 |   jsmith
   5507151954028826462 |  Sivaaaa
   5814973814339647728 | sasidhar
   6921446797511156931 |     UID1
   7900084034148012049 | coder2
   7965147011818932354 |     Anna
   
 - To know which row is in which node
 $ nodetool getendpoints keyspace_name table_name key_column_value
172.31.92.87

