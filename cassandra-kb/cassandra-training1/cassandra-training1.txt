https://dzone.com/articles/introduction-apache-cassandras

TUTORIAL - https://obscure-temple-99364.herokuapp.com/ (install to data-model)
RDBMS
Strict schema
Not natively distributed - distributed is an additional feature
- so, not natively scalable
ACID

NO-SQL
Schema free - dynamic schema
Distributed natively - that is the feature
CAP instead of ACID
- Consistency, Availability, Parititioning
- Consistency is inversely proportional to partitioning
- Performance higher with more partitions

NO SQL CATEGORIES
key-value - redis
document oriented (is actually a row) - mongo, couch, dynamo…
column oriented - key-value - cassandra, hbase 
graph oriented

Mongo - heavy read ok, heavy write not ok
Cassandra - heavy write ok, heavy reads not that ok
Cassandra - does not have a single master node - so more failure tolerant

HBase is Hadoop ‘database’ - like a database designed for Hadoop workloads

CASSANDRA
Multiple machines
Write anywhere - yet user sees the whole thing as a unified one
In mysql, bigtable etc some nodes should be set up as slaves
—> but, cassandra it is decentralized - even the node/master/replication management is distributed
—> peer-to-peer ‘gossip’ protocol to keep in synch with other nodes
—> so, NO SINGLE POINT OF FAILURE - high availability ensured
Nodes can be added and removed dynamically without downtime 
- seamless scale up/down
- no downtime for the cluster overall
Patching within same major versions can be upgraded in rolling fashion

CLUSTER
Data-Centers - DC1,DC2... - for disaster recovery
Each DC with multiple racks and each rack with multiple nodes
Nodes can have different minor-versions within the same major version (like 3.1, 3.2 etc)

CQL - Cassandra - JDK - OS
- CQL is the high level API to access Cassandra
- Cassandra is written in Java
- It will talk to OS via JDK

NODE, DATA CENTER, CLUSTER
https://www.guru99.com/cassandra-architecture.html
Node
Node is the place where data is stored. It is the basic component of Cassandra.

Data Center
A collection of nodes are called data center. Many nodes are categorized as a data center.

Cluster
The cluster is the collection of many data centers.

Commit Log
Every write operation is written to Commit Log. Commit log is used for crash recovery.
- durable-writes=true is the setting that provides this.  True is default for a keyspace
- once flushed to sstable, corresponding commit-log record is set to null/zero (something like that)

Memtable
After data written in Commit log, data is written in Memtable. Data is written in Memtable temporarily (like buffer pool)
https://docs.jboss.org/author/display/RHQ/Cassandra+Configuration+and+Tuning?_sscc=t
flush_largest_memtables_at
memtable_total_space_in_mb
memtable_flush_queue_size
memtable_flush_writers

SSTable
When Memtable reaches a certain threshold, data is flushed to an SSTable disk file.
One SSTable per table
Each flush creates a new SSTable file for a table (with a different serial number)
'Compact' process merges multiple SSTables into one file

TOKENS 
It is the 'vnodes' Cassandra creates within the node - to distribute data within the node.
The number 256 is default which is an optimal benchmkark for 8GB 2CPU Cassandra node.
Higher number of tokens per node can be given for higher capacity machines.

Depending upon number of total machines and their capacities define a total number of tokens 
and then divide them across nodes.

Murmur algorithm does the replica allocation across nodes - no need to manually set the token limit values

REPLICATION
Replication is for a keyspace - not a table or a node

WRITES HOW IT WORKS
First writes go to memory - as a 'memtable'
After the memory threshold is reached - it gets flushed to disk
Then it writes to commit logs and then to table 'sstable' (one sstable for each table - multiple copies per each flush)
sstable compacts periodically (can be set) to compact fragmentation
-- that is, every flush creates a new sstable file - and compact combines all of them into one file with next serial number
-- this is something like - all new rows, and updates between two flushes is parked in intermediate sstables
-- upon compactions mulitple parked sets are merged to get a final view of all rows in those parking spaces
-- each sstable file before compact may have same rows also (different incarnations - insert, update, update... delete)

If memtable threshold is increased much - then if node fails then that much data will be lost and needs recovery

COMPACTION
Compaction-Strategy
Different strategies for different types of data like time-series data etc
Tombstone-deletes
Threshold minimum 4

READS
First looks in memory (like in Oracle) 
Checks whether the node owns the data or not (in key-cache) - then checks in memory (row-cache)
Gets, misses
LRU row-cache

MONITORING
tpstatus

=========================
MULTINODE CLUSTER
=========================
NOTE: For steps, see CREATE CLUSTER section
/etc/cassandra/cassandra.yaml
Bootstrapping

Change these your actual IP Address:
seedIP
ListenAddress
RPC_Address

Autobootstrap = off means it starts as a single node, but ready for bootstrapping into a cluster

Snitch:
Discover topology of the network (like racks, data centers etc)

============
PARTITIONER
============
Murmur3 partition hashes everything into a key of 64bit integer - which will be based on the PK of the record itself.
Then it will distribute it to nodes based on the tokens allocated of each node and some token-range algorithm.
To find where a row actually sits:  nodetool ring <keyspace name>

$ nodetool ring ks0 | wc
1546   12324  189349

NOTE:  The 1546 above is the total tokens/vnodes - 256 tokens per node multiplied by 6 nodes

$ nodetool ring ks0 |head
Datacenter: dc1
==========
Address        Rack        Status State   Load            Owns                Token                                       
                                                                              9222365415242980500                         
172.31.85.72   rack1       Up     Normal  230.18 KiB      16.40%              -9210043340703805705                        
172.31.95.127  rack1       Up     Normal  70.8 KiB        16.38%              -9204249525111820733                        
172.31.85.72   rack1       Up     Normal  230.18 KiB      16.40%              -9201888592744065084                        
172.31.82.133  rack1       Up     Normal  120.17 KiB      16.57%              -9188869267995685337                        
172.31.95.127  rack1       Up     Normal  70.8 KiB        16.38%              -9173690029360339008                        
172.31.84.180  rack1       Up     Normal  355.95 KiB      17.48%              -9145122507948764650  

=========================
DATA CENTERS
=========================

Data Center1 - rack1, rack2
	rack1 - node1a, node1b, node1c, node1d
	rack2 - node1a, node1b, node1c, node1d

Data Center2 - rack1, rack2
	rack1 - node1a, node1b, node1c, node1d
	rack2 - node1a, node1b, node1c, node1d
	
Network topology is understood by 'snitch'

Default configuration dc1,rack1 - one data center, one rack

Cassandra tries to store data across data centers for HADR - partition tolerance
- and tries to route queries to local data centers

--------------
GOSSIP
--------------
Gossip protocol - failure detection - allows each node to keep track of state info about other nodes in the cluster
- runs every second on a timer
- just keeps track of metadata of node-alive - and does not bother about which data is stored where

Seed node knows everything about all nodes
Gossip looks for info in 3 neighboring nodes - and those nodes connect with three other
Gossip verifies for up/down kind of info

Every second a node returns GossipDigestAckMessage and then a return-message is received
- Phi Accrual Failure Detection
- Nodes not responding compared to previously acknowledging-nodes list are considered down

Threshold is possible (like 3 gossips) before declaring a node is down

--------------
SNITCH
--------------
Snitches gather info about network topology
- figure out where nodes are in relation to other nodes

Different snitches for Amazon EC2, Google Cloud, Apache Cloudstack etc

Package org.apache.cassandra.locator
- IEndpointSnitch interface

GossippingPropertyFileSnitch, SimpleSnitch

Dynamic Snitching
- Cassandra will be able to figure out which nodes are how 'far' or same/different network - based on response speeds

--- SOME POINTS TBD
Coordinator nodes
ring vs cluster vs datacenter
In connect string provide local nodes first remote nodes later
Do cross-data center replication
--------

RING - total collection of data in a cluster - which contradicts a bit - that within a datacenter also it is a ring
Token - hash number created for a PK
Virtual Nodes - 256 tokens by default (tokens count = vnode count)
num_token property
More tokens for bigger machines

PARTITIONER - default is murmur3
SimpleStrategy, NetworkTopologyStrategy
NetworkTopologyStrategy - looks for differentdata centers and racks to provide HA-DR replication

-------------------
CONSISTENCY LEVELS
-------------------
Terminologies - consistency level, eventual consistency, tunably consistent system

W=write
R=read
N=replication factor

Formulas:
W+R > N gives strong consistency
Quorum = (N/2) + 1 = Cassandra will automatically use it for both read and write
ALL and ALL is another possibility

Levels: one, two, three - absolute numer of replica nodes that must respond to a request
- applies to read and write (set separately)
The QUORUM consistency level requires response from a majority of the replica nodes (relication factor / 2) + 1 nodes

Write - default consistency is one - so it writes to one node only - and wait for only one node to respond
- fails if that node is down
- (and replication happens in the background)
- have higher consistency for writes so that write to at least one node is successful

Read - default consistency is one - so, if the node is down, reads fail
- if level is higher than one, there could be data mismatch

Replication factor = 1 and consistency > 1 is not possible

Cases: Assume Replication-Factor = 3
wc=1, rc=1 - reads will be fastest as read is from one node, but may return inconsistent data
wc=1, rc=2 - reads will wait for two replicas to give response - what if it reads from two stale replicas?
- Java client drivers are intelligent to get the better consistent one 
-- that intelligence is called 'READ REPAIR' - which accepts the later timestamp and sends message to stale replica
-- THIS NEEDS THE MULTIPLE DATA CENTERS BE IN SAME TIME ZONE or UTC/GMT

* STRONG CONSISTENCY - wc=1, rc=3 - This will be CONSISTENT as all 3 replicas respond and client reads latest from them
- FAST WRITE, SLOW READ

wc=2, rc=2 - does not give much consistency

* STRONG CONSISTENCY - wc=2, rc=2 - with replication factor=3 gives very good consistency
- READ AND WRITE WELL TUNED FOR SPEED AND CONSISTENCY

* STRONG CONSISTENCY - wc=3, rc=1 - strong consitency
- READ FAST, WRITES A BIT SLOW

Questions:
1. If can write to any, but data will sit some place in the ring based on token of the key
- Right

2. Does cassandra have a concept of virtual/scan-ip for the whole cluster, 
   which can avoid us giving multiple-IPs as contact points 
- No, that has to be done by an extenral load-balancer or Fastly and such

3. In reads - instead of giving contact points of local data center - can we give a virtual IP for the whole cluster
   and the cluster can figure out where the query is coming from and direct to local datacenter
- No, that has to be done by an extenral load-balancer or Fastly and such


=========================
ANTI-ENTROPY-REPAIR
=========================
- Manual operation to achieve consistency
Command - nodetool repair

=========================
COORDINATOR NODE
=========================
Among the contact-points one of them acts as a coordinator node to get results according to consistency level

=========================
INTERNAL STRUCTURES
=========================
- MEMORY
memtables, key caches, row caches

- DISK
commit logs, sstables, hints (2.2+)

=========================
CACHING
=========================
Key Cache
- Partition key to row matching

Row Cache
- Rows still available in memory per LRU

Counter Cache
- Counter (like sequence) cache

Hinted Handoff
- If a replica node is not reachable, then a 'hinted handoff' can happen 
- handoff a write to a neighboring node - and then when replica comes up, then synch with it

Lightweight Transactions and Paxos
- Paxos is a consensus algorithm that allows distributed ...
- Prepare/promise, and propose/accept

Tombstones (marked for delete)
- Deletion marker that s required to suppress older data until compaction runs
- Garbage Collection Grace Seconds - GCGraceSeconds - garbage collection done after a tombstone is older than this seconds
- This is a minor-compaction of Tombstones.  A major compaction of them across all nodes needs a planned manual work
- TBD - read and understand more when deletes actually happen, when they never happen and become tombstone etc

Bloom Filters
- To improve read performance
- Uses probablity rule to figure out whether it is in this set or that set
- Map values in a data set in bit arrays and when queried, send the query to the right set

Compaction
SizeTieredCompactionStrategy - default - good for write intensive tables
LeveledCompactionCompactionStrategy - when sstable reaches a size - good for read intensive tables
DateTieredCompactionStrategy - good for time series tables or otherwise date based data

TBD - how memtable flush threshold, tombstone compact and sstable compact parameter to be tuned

=========================
MULTI DATACENTER CLUSTER
=========================
NetworkTopologyStrategy - for replication - it will understand the topology and place replicas in different DC's

File - cassandra-rackdc.properties: 
dc=dc2
rack=rack1

File - cassandra-env.sh:
JVM_OPTS="$JVM_OPTS -Dcassandra.ignore_dc=true"
--> This is required in the case we had this machine in a different DC before and now changing to a new DC

Then, to actually use all this, create or alter keyspace to use NetworkTopologyStrategy and N1 and N2 nodes in each DC.
--> See technical things section for the steps

=========================
DATA MODELING
=========================
TUTORIAL:  https://obscure-temple-99364.herokuapp.com/Model/CrossFit_Gyms.html?q=

NON-GOALS
Minimize writes
Minimize data duplication

GOALS
Spread data evenly around the cluster
Minimize number of partitions read

Model around queries
Determine what queries to support
Try create a table where you can satisfy your query by reading roughly one partition

Keys
Partition key - for distributing by needed category 
- This is used to place the data in specific partitions (token is created based on this)
- Can have composite partitioning key
- If using composite paritioning key, then query should contain all columns of the partition key

Clustering key - for 'order by'
- It further places data in order in a given partition
- Clustering  keys  are  responsible  for  sorting  data  within  a  partition. 

Syntax quirks:
PRIMARY KEY (a) --> a is the PK as well as partition-key
PRIMARY KEY (a, b) --> 'b' is not taken as part of a partition key --> (a,b) is PK, a is partition-key, b is clustering key
PRIMARY KEY ((a, b)) --> now, (a,b) is PK as well as partition-key
PRIMARY KEY ((a, b), c,d,e) --> now, (a,b,c) is PK, (a,b) is partition-key and c,d,e is clustering key

In PRIMARY KEY ((a, b), c,d,e) -
- Default ordering of c,d,e is DESC order
- To customize ordering - add this in create table - WITH CLUSTERING ORDER BY (c desc, d asc, d asc)

CAUTION 1:  
PK becomes partitioning key or a combination of partitioning key and clustering key
And, it is geared towards lookup speed by paritioning and ordering, than maintaining uniqueness 
For example, category in a 'products' table can be a primary key where lookup by 'category' is main thing to achieve
- In such table it is possible to have duplicates on 'product id'

CAUTION 2:
When a key is set, then queries based on non-key columns will not be fast
For such queries, it is advised to create a second table with SAME DATA - but keys for that query
--> This means duplication of data
- but that is to be expected - as Cassandra is not to be used as a single source of truth like ACID RDBMS

EAMPLES OF KEYS:
Example: (bad example)
Product table - make productId as parition key (and PK)
- Uniformly spreads the table
- But, if the query is based on product category then minimize number of partitions read will be violated
-- because ID does not care for category - so each product category may be sitting in many partitions
-- If the key was product-category, then each node would potentially carry same categories and not spreads it on other nodes

Example: (good example)
Partitioning Key = brand (like apple, dell) --> Product arranged in partitions by brand
Clustering Key = product type (like mobiles, laptops etc) --> Within parititon arranged by product type

Example: (better example) - because customers shop for product-types and not brands 
Paritioning key = product type
Clustering key = brand

Problem:  
A product can belong to multiple product category - like baby and clothing (two separate categories)
And, also it could be multiple brands as well
- then we cannot satisfy both categories in one data model for this data

Viewing the tokens of a key: (assuming the three columns below comprise paritition key)
SELECT token(country_code, state_province, city) FROM crossfit_gyms_by_city;

 system.token(country_code, state_province, city)
--------------------------------------------------
                             -8458359574911311540
                             -8276155982978925814
                             -3658271257738167517
                             -2492327001946213135
                             -2107058130455252847


DATA TYPES:
Supports binary data in blob - but not recommended to keep large info in such columns.
List, map (kv), set, inet (IP addr) are other special data types

MATERIALIZED VIEW:


BATCH INSERT:
If one insert fails, will the whole batch fail??

INDEX:
Indexes can be created on a column that is not part of partitioning key or clustering keys.
/var/lib/cassandra/saved_caches keeps the disk flushed indexes (managed differently compared to table flushing)
Index mainly is kept in memory
Index in a partition keeps pointers to rows in the partition
When quried based on the index, all partitions will have to be looked up
Do not create indexes on frequently updated columns

======================
CASSANDRA-STRESS TOOL
======================
Workloads - read, write, mixed

Creates a keyspace called keyspace1 with single-DC - and emulates some load.

cassandra-stress.yaml

===================
TIME TO LIVE
===================
Columns and tables support an optional expiration period called TTL (time-to-live); 
TTL is not supported on counter columns. Define the TTL value in seconds. 
Data expires once it exceeds the TTL period and is then marked with a tombstone. 
Expired data continues to be available for read requests during the grace period, see gc_grace_seconds. 
Normal compaction and repair processes automatically remove the tombstone data.

default_time_to_live is 0 - when we describe a table you can see this setting.

Both the INSERT and UPDATE commands support setting a time for data in a column to expire. Use CQL to set the expiration time (TTL).

Procedure

Use the INSERT command to set calendar listing in the calendar table to expire in 86400 seconds, or one day.

cqlsh> INSERT INTO cycling.calendar (race_id, race_name, race_start_date, race_end_date) VALUES (200, 'placeholder', '2015-05-27', '2015-05-27') USING TTL 86400;

Extend the expiration period to three days by using the UPDATE command and change the race name.

cqlsh> UPDATE cycling.calendar USING TTL 259200 
  SET race_name = 'Tour de France - Stage 12' 
  WHERE race_id = 200 AND race_start_date = '2015-05-27' AND race_end_date = '2015-05-27';
  
Delete a column's existing TTL by setting its value to zero.

cqlsh> UPDATE cycling.calendar USING TTL 0 
  SET race_name = 'Tour de France - Stage 12' 
  WHERE race_id = 200 AND race_start_date = '2015-05-27' AND race_end_date = '2015-05-27';
You can set a default TTL for an entire table by setting the table's default_time_to_live property. If you try to set a TTL for a specific column that is longer than the time defined by the table TTL, Cassandra returns an error.

=========================
COMPACTION STRATEGY
=========================
alter table cyclists with compaction - ......

Describe table shows the compaction strategy in force for the table.

=========================
COMPRESSION
=========================
Describe table shows the compression strategy in force for the table.
enable/disable - default is enable - to reduce overall space usage as we will have multiple copies
Default LZ4Compression is used in write/read heavy tables

===========================
MEMTABLE_FLUSH_PERIOD_IN_MS
===========================
At table level
Default is 0 - then it lushes in shutdown, flush, and when memtable memory space becomes full
Change it to enable flush at that desired periodicity

==============================
ENGINES AND MANAGERS/SERVICES
==============================
Engines
Storage engine
Stream manager - read repair, anti-entropy repair
CQL Native transport service

Managers and Services
Repair
Caching
Migration
Materialized views
Secondary indexes
Authorization

===========================
BACKUP AND RESTORE
===========================
Repairs - read repair, anti-entropy repair
nodetool repair --> triggers major compaction

Snapshots
- Nodewide only - not clusterwide (so, the clusterwide backup means taking backup of each node)
- Copied into a folder
- Per table
- "nodetool snapshot"

# nodetool snapshot
Requested creating snapshot(s) for [all keyspaces] with snapshot name [1530870387569] and options {skipFlush=false}
Snapshot directory: 1530870387569

Incremental Backups
- Creates snapshots for each new flush of SSTable to backup the directory automatically
- Disabled by default
-- to enable, set incremental_backups = true in cassandra.yaml

===========================
MONITORING
===========================
Datadog, Prometheus+Grafana

- CASSANDRA MONITORING COMMANDS

# nodetool proxyhistograms --> shows the network statistics in a cluster.
# nodetool proxyhistograms
proxy histograms
Percentile       Read Latency      Write Latency      Range Latency   CAS Read Latency  CAS Write Latency View Write Latency
                     (micros)           (micros)           (micros)           (micros)           (micros)           (micros)
50%                      0.00               0.00               0.00               0.00               0.00               0.00
75%                      0.00               0.00               0.00               0.00               0.00               0.00
95%                      0.00               0.00               0.00               0.00               0.00               0.00
98%                      0.00               0.00               0.00               0.00               0.00               0.00
99%                      0.00               0.00               0.00               0.00               0.00               0.00
Min                      0.00               0.00               0.00               0.00               0.00               0.00
Max                      0.00               0.00               0.00               0.00               0.00               0.00

# nodetool tablestats
Total number of tables: 47
----------------
----------------
Keyspace : cyclingx
	Read Count: 0
	Read Latency: NaN ms
	Write Count: 0
	Write Latency: NaN ms
	Pending Flushes: 0
		Table: cyclist_name
		SSTable count: 0
		Space used (live): 0
		Space used (total): 0
		Space used by snapshots (total): 0
		Off heap memory used (total): 0
		SSTable Compression Ratio: -1.0
		Number of partitions (estimate): 0
		Memtable cell count: 0
		Memtable data size: 0
		Memtable off heap memory used: 0
		Memtable switch count: 0
		Local read count: 0
		Local read latency: NaN ms
		Local write count: 0
		Local write latency: NaN ms
		Pending flushes: 0
		Percent repaired: 100.0
		Bloom filter false positives: 0
		Bloom filter false ratio: 0.00000
		Bloom filter space used: 0
		Bloom filter off heap memory used: 0
		Index summary off heap memory used: 0
		Compression metadata off heap memory used: 0
		Compacted partition minimum bytes: 0
		Compacted partition maximum bytes: 0
		Compacted partition mean bytes: 0
		Average live cells per slice (last five minutes): NaN
		Maximum live cells per slice (last five minutes): 0
		Average tombstones per slice (last five minutes): NaN
		Maximum tombstones per slice (last five minutes): 0
		Dropped Mutations: 0
		
# nodetool netstats -H
Mode: NORMAL
Not sending any streams.
Read Repair Statistics:
Attempted: 0
Mismatch (Blocking): 0
Mismatch (Background): 0
Pool Name                    Active   Pending      Completed   Dropped
Large messages                  n/a         0              0         0
Small messages                  n/a         0             10         0
Gossip messages                 n/a         0          34760         0

# nodetool tablehistograms simplex songs
simplex/songs histograms
Percentile  SSTables     Write Latency      Read Latency    Partition Size        Cell Count
                              (micros)          (micros)           (bytes)                  
50%             0.00              0.00              0.00               124                 5
75%             0.00              0.00              0.00               124                 5
95%             0.00              0.00              0.00               124                 5
98%             0.00              0.00              0.00               124                 5
99%             0.00              0.00              0.00               124                 5
Min             0.00              0.00              0.00               104                 5
Max             0.00              0.00              0.00               124                 5

# nodetool status
Datacenter: dc1
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address        Load       Tokens       Owns    Host ID                               Rack
UN  172.31.90.156  10.76 MiB  256          ?       6570d39b-3af4-4d5a-9c21-ab694f563f4b  rack1
UN  172.31.92.87   10.84 MiB  256          ?       8f2f9d78-a33c-47f9-90b5-811d9670d410  rack1
Datacenter: dc2
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address        Load       Tokens       Owns    Host ID                               Rack
UN  172.31.95.127  10.64 MiB  256          ?       66002181-ba36-4cfe-9b3a-d165139440ff  rack1
UN  172.31.85.72   10.85 MiB  256          ?       b23352ce-bd97-4713-983a-a0715620a4d7  rack1
UN  172.31.82.133  10.81 MiB  256          ?       6e78e72c-b585-4d9a-861f-62d4dd693a31  rack1
UN  172.31.84.180  11.22 MiB  256          ?       b8b5514e-6a64-45b8-a150-b700769063ab  rack1

Note: Non-system keyspaces don't have the same replication settings, effective ownership information is meaningless

# nodetool info
ID                     : b23352ce-bd97-4713-983a-a0715620a4d7
Gossip active          : true
Thrift active          : false
Native Transport active: true
Load                   : 10.85 MiB
Generation No          : 1530857131
Uptime (seconds)       : 10061
Heap Memory (MB)       : 107.76 / 990.00
Off Heap Memory (MB)   : 0.25
Data Center            : dc2
Rack                   : rack1
Exceptions             : 0
Key Cache              : entries 58, size 4.59 KiB, capacity 49 MiB, 79 hits, 143 requests, 0.552 recent hit rate, 14400 save period in seconds
Row Cache              : entries 0, size 0 bytes, capacity 0 bytes, 0 hits, 0 requests, NaN recent hit rate, 0 save period in seconds
Counter Cache          : entries 0, size 0 bytes, capacity 24 MiB, 0 hits, 0 requests, NaN recent hit rate, 7200 save period in seconds
Chunk Cache            : entries 10, size 640 KiB, capacity 215 MiB, 64 misses, 234 requests, 0.726 recent hit rate, NaN microseconds miss latency
Percent Repaired       : 0.03129704148589808%
Token                  : (invoke with -T/--tokens to see all 256 tokens)

# nodetool tpstats (Thread Pools Statistics)
# nodetool tpstats
Pool Name                         Active   Pending      Completed   Blocked  All time blocked
ReadStage                              0         0              6         0                 0
MiscStage                              0         0              0         0                 0
CompactionExecutor                     0         0           7575         0                 0
MutationStage                          0         0             65         0                 0
MemtableReclaimMemory                  0         0             27         0                 0
PendingRangeCalculator                 0         0              6         0                 0
GossipStage                            0         0          32683         0                 0
SecondaryIndexManagement               0         0              0         0                 0
HintsDispatcher                        0         0              0         0                 0
RequestResponseStage                   0         0              5         0                 0
Native-Transport-Requests              0         0              0         0                 0
ReadRepairStage                        0         0              0         0                 0
CounterMutationStage                   0         0              0         0                 0
MigrationStage                         0         0              0         0                 0
MemtablePostFlush                      0         0             28         0                 0
PerDiskMemtableFlushWriter_0           0         0             27         0                 0
ValidationExecutor                     0         0              0         0                 0
Sampler                                0         0              0         0                 0
MemtableFlushWriter                    0         0             27         0                 0
InternalResponseStage                  0         0              0         0                 0
ViewMutationStage                      0         0              0         0                 0
AntiEntropyStage                       0         0              0         0                 0
CacheCleanupExecutor                   0         0              0         0                 0

Message type           Dropped
READ                         0
RANGE_SLICE                  0
_TRACE                       0
HINT                         0
MUTATION                     0
COUNTER_MUTATION             0
BATCH_STORE                  0
BATCH_REMOVE                 0
REQUEST_RESPONSE             0
PAGED_RANGE                  0
READ_REPAIR                  0






== ********* == == ********* == == ********* == == ********* == == ********* == == ********* == 
===============================================================================================
=========================
TECHNICAL THINGS
=========================
COMMANDS, FILES ETC

/var/log/cassandra
/var/lib/cassandra
/var/lib/cassandra/data
/etc/cassandra/cassandra.yaml

nodetool status
cqlsh

=========================
INSTALL CONFIG on Ubuntu
=========================
# echo "deb http://www.apache.org/dist/cassandra/debian 311x main" | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list

# curl https://www.apache.org/dist/cassandra/KEYS | sudo apt-key add -

# apt-get update
# apt-get install cassandra  -y

# which cassandra
/usr/sbin/cassandra

# which cqlsh
/usr/bin/cqlsh

# service  cassandra status
 * Cassandra is running
 
# ls /var/lib/cassandra/
commitlog  data  hints  saved_caches

# ls /var/lib/cassandra/data/
system  system_auth  system_distributed  system_schema  system_traces

# tail /var/log/cassandra/system.log

# nodetool status
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load       Tokens       Owns (effective)  Host ID                               Rack
UN  127.0.0.1  103.67 KiB  256          100.0%            5975effd-cd85-4f8e-a83c-c66bf7840689  rack1

$ cqlsh
Connected to Test Cluster at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 3.11.2 | CQL spec 3.4.4 | Native protocol v4]
Use HELP for help.
cqlsh> describe keyspaces

system_traces  system_schema  system_auth  system  system_distributed

cqlsh> create keyspace ks0 with replication = {'class':'SimpleStrategy','replication_factor':1};
cqlsh> describe keyspace ks0

CREATE KEYSPACE ks0 WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'}  AND durable_writes = true;

cqlsh> use ks0;
cqlsh:ks0> 

cqlsh:ks0> CREATE TABLE users(user_id varchar, age int, email varchar, city varchar, PRIMARY KEY (user_id));
cqlsh:ks0> describe ks0.users

CREATE TABLE ks0.users (
    user_id text PRIMARY KEY,
    age int,
    city text,
    email text
) WITH bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';
    
cqlsh:ks0> INSERT INTO users(user_id, age, email, city) VALUES ('jsmith',32,'john.smith@example.com','Dallas');
cqlsh:ks0> 
cqlsh:ks0> select * from users;

 user_id | age | city   | email
---------+-----+--------+------------------------
  jsmith |  32 | Dallas | john.smith@example.com

(1 rows)
cqlsh:ks0> 

$ nodetool status
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load       Tokens       Owns (effective)  Host ID                               Rack
UN  127.0.0.1  200.17 KiB  256          100.0%            5975effd-cd85-4f8e-a83c-c66bf7840689  rack1

$ nodetool status admatic24

Error: The keyspace admatic24, does not exist

$ nodetool status ks0
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load       Tokens       Owns (effective)  Host ID                               Rack
UN  127.0.0.1  200.17 KiB  256          100.0%            5975effd-cd85-4f8e-a83c-c66bf7840689  rack1

$ nodetool info
ID                     : 5975effd-cd85-4f8e-a83c-c66bf7840689
Gossip active          : true
Thrift active          : false
Native Transport active: true
Load                   : 200.17 KiB
Generation No          : 1530523374
Uptime (seconds)       : 83348
Heap Memory (MB)       : 224.70 / 990.00
Off Heap Memory (MB)   : 0.00
Data Center            : datacenter1
Rack                   : rack1
Exceptions             : 0
Key Cache              : entries 27, size 2.34 KiB, capacity 49 MiB, 130 hits, 168 requests, 0.774 recent hit rate, 14400 save period in seconds
Row Cache              : entries 0, size 0 bytes, capacity 0 bytes, 0 hits, 0 requests, NaN recent hit rate, 0 save period in seconds
Counter Cache          : entries 0, size 0 bytes, capacity 24 MiB, 0 hits, 0 requests, NaN recent hit rate, 7200 save period in seconds
Chunk Cache            : entries 20, size 1.25 MiB, capacity 215 MiB, 222 misses, 542 requests, 0.590 recent hit rate, NaN microseconds miss latency
Percent Repaired       : 100.0%
Token                  : (invoke with -T/--tokens to see all 256 tokens)

$ nodetool cfstats ks0
Total number of tables: 37
----------------
Keyspace : ks0
	Read Count: 0
	Read Latency: NaN ms
	Write Count: 8
	Write Latency: 0.09575 ms
	Pending Flushes: 0
		Table: users
		SSTable count: 1
		Space used (live): 5237
		Space used (total): 5237
		Space used by snapshots (total): 0
		Off heap memory used (total): 51
		SSTable Compression Ratio: 0.31970260223048325
		Number of partitions (estimate): 8
		Memtable cell count: 0
		Memtable data size: 0
		Memtable off heap memory used: 0
		Memtable switch count: 4
		Local read count: 0
		Local read latency: NaN ms
		Local write count: 8
		Local write latency: NaN ms
		Pending flushes: 0
		Percent repaired: 0.0
		Bloom filter false positives: 0
		Bloom filter false ratio: 0.00000
		Bloom filter space used: 24
		Bloom filter off heap memory used: 16
		Index summary off heap memory used: 19
		Compression metadata off heap memory used: 16
		Compacted partition minimum bytes: 61
		Compacted partition maximum bytes: 72
		Compacted partition mean bytes: 72
		Average live cells per slice (last five minutes): NaN
		Maximum live cells per slice (last five minutes): 0
		Average tombstones per slice (last five minutes): NaN
		Maximum tombstones per slice (last five minutes): 0
		Dropped Mutations: 0

----------------

DO SOME INSERTS
cqlsh:ks0> INSERT INTO users(user_id, age, email, city) VALUES ('jsmith9',32,'john.smith@example.com','Dallas');
cqlsh:ks0> INSERT INTO users(user_id, age, email, city) VALUES ('jsmith10',32,'john.smith@example.com','Dallas');
cqlsh:ks0> INSERT INTO users(user_id, age, email, city) VALUES ('jsmith11',32,'john.smith@example.com','Dallas');

CHECK MEMTABLE CHANGES
$ nodetool cfstats ks0
Total number of tables: 37
----------------
Keyspace : ks0
	Read Count: 0
	Read Latency: NaN ms
	Write Count: 11
	Write Latency: 0.08963636363636364 ms
	Pending Flushes: 0
		Table: users
		SSTable count: 1
		Space used (live): 5237
		Space used (total): 5237
		Space used by snapshots (total): 0
		Off heap memory used (total): 51
		SSTable Compression Ratio: 0.31970260223048325
		Number of partitions (estimate): 11
		Memtable cell count: 3
		Memtable data size: 327
		Memtable off heap memory used: 0
		Memtable switch count: 4
		Local read count: 0
		Local read latency: NaN ms
		Local write count: 11
		Local write latency: 0.085 ms
		Pending flushes: 0
		Percent repaired: 0.0
		Bloom filter false positives: 0
		Bloom filter false ratio: 0.00000
		Bloom filter space used: 24
		Bloom filter off heap memory used: 16
		Index summary off heap memory used: 19
		Compression metadata off heap memory used: 16
		Compacted partition minimum bytes: 61
		Compacted partition maximum bytes: 72
		Compacted partition mean bytes: 72
		Average live cells per slice (last five minutes): NaN
		Maximum live cells per slice (last five minutes): 0
		Average tombstones per slice (last five minutes): NaN
		Maximum tombstones per slice (last five minutes): 0
		Dropped Mutations: 0

----------------

FLUSH
$ nodetool flush

CHECK MEMTABLE CHANGES
$ nodetool cfstats ks0
Total number of tables: 37
----------------
Keyspace : ks0
	Read Count: 0
	Read Latency: NaN ms
	Write Count: 11
	Write Latency: 0.08963636363636364 ms
	Pending Flushes: 0
		Table: users
		SSTable count: 2
		Space used (live): 10313
		Space used (total): 10313
		Space used by snapshots (total): 0
		Off heap memory used (total): 86
		SSTable Compression Ratio: 0.3795918367346939
		Number of partitions (estimate): 11
		Memtable cell count: 0
		Memtable data size: 0
		Memtable off heap memory used: 0
		Memtable switch count: 5
		Local read count: 0
		Local read latency: NaN ms
		Local write count: 11
		Local write latency: NaN ms
		Pending flushes: 0
		Percent repaired: 0.0
		Bloom filter false positives: 0
		Bloom filter false ratio: 0.00000
		Bloom filter space used: 40
		Bloom filter off heap memory used: 24
		Index summary off heap memory used: 38
		Compression metadata off heap memory used: 24
		Compacted partition minimum bytes: 61
		Compacted partition maximum bytes: 72
		Compacted partition mean bytes: 72
		Average live cells per slice (last five minutes): NaN
		Maximum live cells per slice (last five minutes): 0
		Average tombstones per slice (last five minutes): NaN
		Maximum tombstones per slice (last five minutes): 0
		Dropped Mutations: 0

----------------

$ cqlsh
Connected to Test Cluster at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 3.11.2 | CQL spec 3.4.4 | Native protocol v4]
Use HELP for help.

cqlsh> use ks0 ;
cqlsh:ks0> INSERT INTO users(user_id, age, email, city) VALUES ('jsmith9',32,'john.smith@example.com','Dallas');
cqlsh:ks0> INSERT INTO users(user_id, age, email, city) VALUES ('jsmith10',32,'john.smith@example.com','Dallas');
cqlsh:ks0> INSERT INTO users(user_id, age, email, city) VALUES ('jsmith11',32,'john.smith@example.com','Dallas');

$ nodetool cfstats ks0
Total number of tables: 46
----------------
Keyspace : ks0
	Read Count: 0
	Read Latency: NaN ms
	Write Count: 13
	Write Latency: 0.08592307692307692 ms
	Pending Flushes: 0
		Table: emp
		SSTable count: 2
		Space used (live): 10499
		Space used (total): 10499
		Space used by snapshots (total): 0
		Off heap memory used (total): 80
		SSTable Compression Ratio: 0.5707692307692308
		Number of partitions (estimate): 10
		Memtable cell count: 1
		Memtable data size: 112
		Memtable off heap memory used: 0
		Memtable switch count: 4
		Local read count: 0
		Local read latency: NaN ms
		Local write count: 13
		Local write latency: 0.219 ms
		Pending flushes: 0
		Percent repaired: 0.0
		Bloom filter false positives: 0
		Bloom filter false ratio: 0.00000
		Bloom filter space used: 40
		Bloom filter off heap memory used: 24
		Index summary off heap memory used: 32
		Compression metadata off heap memory used: 24
		Compacted partition minimum bytes: 51
		Compacted partition maximum bytes: 60
		Compacted partition mean bytes: 60
		Average live cells per slice (last five minutes): NaN
		Maximum live cells per slice (last five minutes): 0
		Average tombstones per slice (last five minutes): NaN
		Maximum tombstones per slice (last five minutes): 0
		Dropped Mutations: 0

----------------
$ nodetool compactionstats
pending tasks: 0

$
$ ls /var/lib/cassandra/data/ks0/users-c0d79fb05d7a11e8a4c9d52e94b3f8eb/
backups                      mc-4-big-Index.db            mc-5-big-Data.db        mc-5-big-Summary.db
mc-4-big-CompressionInfo.db  mc-4-big-Statistics.db       mc-5-big-Digest.crc32   mc-5-big-TOC.txt
mc-4-big-Data.db             mc-4-big-Summary.db          mc-5-big-Filter.db
mc-4-big-Digest.crc32        mc-4-big-TOC.txt             mc-5-big-Index.db
mc-4-big-Filter.db           mc-5-big-CompressionInfo.db  mc-5-big-Statistics.db

$ nodetool compactionstats
pending tasks: 0

$ sudo nodetool compact

$ sudo nodetool compact
$ nodetool compactionstats
pending tasks: 0

$ nodetool gcstats
       Interval (ms) Max GC Elapsed (ms)Total GC Elapsed (ms)Stdev GC Elapsed (ms)   GC Reclaimed (MB)         Collections      Direct Memory Bytes
            80388500                 199                1557                  22          5920033952                  72                       -1

$ nodetool tpstats
Pool Name                         Active   Pending      Completed   Blocked  All time blocked
ReadStage                              0         0            376         0                 0
MiscStage                              0         0              0         0                 0
CompactionExecutor                     0         0          50080         0                 0
MutationStage                          0         0             20         0                 0
MemtableReclaimMemory                  0         0            134         0                 0
PendingRangeCalculator                 0         0              1         0                 0
GossipStage                            0         0              0         0                 0
SecondaryIndexManagement               0         0              0         0                 0
HintsDispatcher                        0         0              0         0                 0
RequestResponseStage                   0         0              0         0                 0
Native-Transport-Requests              0         0           2116         0                 0
ReadRepairStage                        0         0              0         0                 0
CounterMutationStage                   0         0              0         0                 0
MigrationStage                         0         0             21         0                 0
MemtablePostFlush                      0         0            379         0                 0
PerDiskMemtableFlushWriter_0           0         0            134         0                 0
ValidationExecutor                     0         0              0         0                 0
Sampler                                0         0              0         0                 0
MemtableFlushWriter                    0         0            134         0                 0
InternalResponseStage                  0         0              0         0                 0
ViewMutationStage                      0         0              0         0                 0
AntiEntropyStage                       0         0              0         0                 0
CacheCleanupExecutor                   0         0              0         0                 0

Message type           Dropped
READ                         0
RANGE_SLICE                  0
_TRACE                       0
HINT                         0
MUTATION                     0
COUNTER_MUTATION             0
BATCH_STORE                  0
BATCH_REMOVE                 0
REQUEST_RESPONSE             0
PAGED_RANGE                  0
READ_REPAIR                  0
$

=======================
CREATE CLUSTER
=======================

--------------------------------
CREATE WITH ONLY ONE SEED
--------------------------------

- Configure
On each node, edit /etc/cassandra/cassandra.yml.
Give the private IP of the seed node against seeds.
Give the private IP of the local node against listen_address and rpc_address.

-- Settings to be done in cassandra.yml

# diff cassandra.yaml.orig cassandra.yaml
10c10
< cluster_name: 'Test Cluster'
---
> cluster_name: 'Team4Cluster'
425c425
<           - seeds: "127.0.0.1"
---
>           - seeds: "172.31.84.180"
599c599
< listen_address: localhost
---
> listen_address: 172.31.85.72
676c676
< rpc_address: localhost
---
> rpc_address: 172.31.85.72
949c949
< endpoint_snitch: SimpleSnitch
---
> endpoint_snitch: GossipingPropertyFileSnitch
1237a1238,1239
> #
> auto_bootstrap: false

- Remove the folder /var/lib/cassandra/data

- Verify

$ nodetool status
Datacenter: dc1
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address        Load       Tokens       Owns    Host ID                               Rack
UN  172.31.95.127  70.8 KiB   256          ?       66002181-ba36-4cfe-9b3a-d165139440ff  rack1
UN  172.31.92.87   155.1 KiB  256          ?       8f2f9d78-a33c-47f9-90b5-811d9670d410  rack1
UN  172.31.82.133  120.17 KiB  256          ?       da0d2899-c2c2-4bb3-8267-a590c0ea12ff  rack1
UN  172.31.84.180  355.95 KiB  256          ?       3b6e8368-9274-4238-8a6c-3ff1fb7bd4b1  rack1

Note: Non-system keyspaces don't have the same replication settings, effective ownership information is meaningless

# nodetool status godisgreat
Datacenter: dc1
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address        Load       Tokens       Owns (effective)  Host ID                               Rack
UN  172.31.85.72   170.09 KiB  256          25.2%             b23352ce-bd97-4713-983a-a0715620a4d7  rack1
UN  172.31.92.87   135.23 KiB  256          24.8%             8f2f9d78-a33c-47f9-90b5-811d9670d410  rack1
UN  172.31.82.133  142.94 KiB  256          24.1%             da0d2899-c2c2-4bb3-8267-a590c0ea12ff  rack1
UN  172.31.84.180  143.17 KiB  256          25.8%             3b6e8368-9274-4238-8a6c-3ff1fb7bd4b1  rack1

- Bring down one node and see status
$ nodetool status godisgreat
Datacenter: dc1
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address        Load       Tokens       Owns (effective)  Host ID                               Rack
UN  172.31.95.127  70.8 KiB   256          16.4%             66002181-ba36-4cfe-9b3a-d165139440ff  rack1
UN  172.31.90.156  193.64 KiB  256          16.5%             6570d39b-3af4-4d5a-9c21-ab694f563f4b  rack1
UN  172.31.85.72   230.18 KiB  256          16.4%             b23352ce-bd97-4713-983a-a0715620a4d7  rack1
UN  172.31.92.87   155.1 KiB  256          16.6%             8f2f9d78-a33c-47f9-90b5-811d9670d410  rack1
UN  172.31.82.133  120.17 KiB  256          16.6%             da0d2899-c2c2-4bb3-8267-a590c0ea12ff  rack1
DN  172.31.84.180  243.7 KiB  256          17.5%             3b6e8368-9274-4238-8a6c-3ff1fb7bd4b1  rack1

- Connect and run a create table
Note: You now need to give IP address as loopback will not work anymore
$ cqlsh 172.31.85.72

- IF THE SEED SERVER(s) ITSELF IS DOWN
You will get NoHostAvailable error even though other machines are up.

cqlsh:godisgreat> select * from users;
NoHostAvailable: 

- To know which row is in which token (combine this with nodetool status ring output to know on which node)
cqlsh:godisgreat> select token(user_id), user_id from users;

 system.token(user_id) | user_id
-----------------------+----------
  -7517536897081353922 |     kris
   -400498980410088294 |     UID2
   -280105036964296635 | coder1
   3387803449176249109 |   jsmith
   5507151954028826462 |  Sivaaaa
   5814973814339647728 | sasidhar
   6921446797511156931 |     UID1
   7900084034148012049 | coder2
   7965147011818932354 |     Anna
   
 - To know which row is in which node
 $ nodetool getendpoints keyspace_name table_name key_column_value
172.31.92.87

--------------------------
MULTI DATACENTER
--------------------------
https://docs.datastax.com/en/cassandra/3.0/cassandra/operations/opsChangeKSStrategy.html

cqlsh> SELECT * FROM system_schema.keyspaces; 

 keyspace_name      | durable_writes | replication
--------------------+----------------+-------------------------------------------------------------------------------------
        system_auth |           True | {'class': 'org.apache.cassandra.locator.SimpleStrategy', 'replication_factor': '1'}
      system_schema |           True |                             {'class': 'org.apache.cassandra.locator.LocalStrategy'}
 system_distributed |           True | {'class': 'org.apache.cassandra.locator.SimpleStrategy', 'replication_factor': '3'}
             system |           True |                             {'class': 'org.apache.cassandra.locator.LocalStrategy'}
       testkeyspace |           True | {'class': 'org.apache.cassandra.locator.SimpleStrategy', 'replication_factor': '1'}
      system_traces |           True | {'class': 'org.apache.cassandra.locator.SimpleStrategy', 'replication_factor': '2'}
            simplex |           True | {'class': 'org.apache.cassandra.locator.SimpleStrategy', 'replication_factor': '1'}
           cyclingx |           True | {'class': 'org.apache.cassandra.locator.SimpleStrategy', 'replication_factor': '1'}


cqlsh> alter keyspace simplex WITH REPLICATION = {'class' : 'NetworkTopologyStrategy', 'DC1' : 2, 'DC2' : 2};
Warning: schema version mismatch detected; check the schema versions of your nodes in system.local and system.peers.

To remove a node temporarily - when it does not bootstrap properly:
# nodetool removenme node_ID 
--> After that, restart cassandra on that offending node and it should come up properly

CONSISTENCY LEVELS

Consistency=1 fast
Consistency=quorum medium
Consistency=all slow

cqlsh> create keyspace ks1 WITH REPLICATION = {'class' : 'NetworkTopologyStrategy', 'DC1' : 2, 'DC2' : 2} ;
cqlsh> use ks1;
cqlsh:ks1> create table tester (id int PRIMARY KEY, col1 int, col2 int);

cqlsh> CREATE KEYSPACE cycling_alt WITH replication = {'class':'SimpleStrategy', 'replicati
on_factor':3};
cqlsh> USE cycling_alt;
cqlsh> CREATE TABLE cycling_alt.tester (id int PRIMARY KEY, col1 int, col2 int);
cqlsh> INSERT INTO cycling_alt.tester(id, col1, col2) VALUES(0, 0, 0);
InvalidRequest: Error from server: code=2200 [Invalid query] message="You must use c
onditional updates for serializable writes"

cqlsh> TRACING on;
cqlsh> CONSISTENCY ONE;
cqlsh:cycling_alt> consistency;
Current consistency level is ONE.
cqlsh:cycling_alt> SELECT * FROM cycling_alt.tester WHERE id = 0;

 id | col1 | col2
----+------+------
  0 |    0 |    0

(1 rows)

Tracing session: ef2a62e0-8033-11e8-8621-0f4156f9c36b

 activity                                                                             | timestamp                  | source       | source_elapsed | client
--------------------------------------------------------------------------------------+----------------------------+--------------+----------------+--------------
                                                                   Execute CQL3 query | 2018-07-05 09:15:17.902000 | 172.31.85.72 |              0 | 172.31.85.72
 Parsing SELECT * FROM cycling_alt.tester WHERE id = 0; [Native-Transport-Requests-1] | 2018-07-05 09:15:17.903000 | 172.31.85.72 |            614 | 172.31.85.72
                                    Preparing statement [Native-Transport-Requests-1] | 2018-07-05 09:15:17.903000 | 172.31.85.72 |            772 | 172.31.85.72
                             Executing single-partition query on tester [ReadStage-2] | 2018-07-05 09:15:17.904000 | 172.31.85.72 |           1347 | 172.31.85.72
                                           Acquiring sstable references [ReadStage-2] | 2018-07-05 09:15:17.904000 | 172.31.85.72 |           1401 | 172.31.85.72
                                              Merging memtable contents [ReadStage-2] | 2018-07-05 09:15:17.904000 | 172.31.85.72 |           1437 | 172.31.85.72
                                 Read 1 live rows and 0 tombstone cells [ReadStage-2] | 2018-07-05 09:15:17.904000 | 172.31.85.72 |           1591 | 172.31.85.72
                                                                     Request complete | 2018-07-05 09:15:17.904345 | 172.31.85.72 |           2345 | 172.31.85.72


cqlsh:cycling_alt> 
cqlsh:cycling_alt> 
cqlsh:cycling_alt> CONSISTENCY quorum;
Consistency level set to QUORUM.
cqlsh:cycling_alt> SELECT * FROM cycling_alt.tester WHERE id = 0;

 id | col1 | col2
----+------+------
  0 |    0 |    0

(1 rows)

Tracing session: 0bab71c0-8034-11e8-8621-0f4156f9c36b

 activity                                                                                          | timestamp                  | source        | source_elapsed | client
---------------------------------------------------------------------------------------------------+----------------------------+---------------+----------------+--------------
                                                                                Execute CQL3 query | 2018-07-05 09:16:05.724000 |  172.31.85.72 |              0 | 172.31.85.72
              Parsing SELECT * FROM cycling_alt.tester WHERE id = 0; [Native-Transport-Requests-1] | 2018-07-05 09:16:05.724000 |  172.31.85.72 |            166 | 172.31.85.72
                                                 Preparing statement [Native-Transport-Requests-1] | 2018-07-05 09:16:05.724000 |  172.31.85.72 |            290 | 172.31.85.72
                                  reading digest from /172.31.95.127 [Native-Transport-Requests-1] | 2018-07-05 09:16:05.725000 |  172.31.85.72 |            631 | 172.31.85.72
                                          Executing single-partition query on tester [ReadStage-2] | 2018-07-05 09:16:05.725000 |  172.31.85.72 |            927 | 172.31.85.72
                                                        Acquiring sstable references [ReadStage-2] | 2018-07-05 09:16:05.725000 |  172.31.85.72 |            994 | 172.31.85.72
                                                           Merging memtable contents [ReadStage-2] | 2018-07-05 09:16:05.729000 |  172.31.85.72 |           4398 | 172.31.85.72
                                              Read 1 live rows and 0 tombstone cells [ReadStage-2] | 2018-07-05 09:16:05.729000 |  172.31.85.72 |           4569 | 172.31.85.72
                            speculating read retry on /172.31.82.133 [Native-Transport-Requests-1] | 2018-07-05 09:16:05.729000 |  172.31.85.72 |           4932 | 172.31.85.72
           Sending READ message to /172.31.95.127 [MessagingService-Outgoing-/172.31.95.127-Small] | 2018-07-05 09:16:05.729000 |  172.31.85.72 |           5048 | 172.31.85.72
           Sending READ message to /172.31.82.133 [MessagingService-Outgoing-/172.31.82.133-Small] | 2018-07-05 09:16:05.729000 |  172.31.85.72 |           5273 | 172.31.85.72
                READ message received from /172.31.85.72 [MessagingService-Incoming-/172.31.85.72] | 2018-07-05 09:16:05.730000 | 172.31.95.127 |             13 | 172.31.85.72
                READ message received from /172.31.85.72 [MessagingService-Incoming-/172.31.85.72] | 2018-07-05 09:16:05.731000 | 172.31.82.133 |             29 | 172.31.85.72
  REQUEST_RESPONSE message received from /172.31.95.127 [MessagingService-Incoming-/172.31.95.127] | 2018-07-05 09:16:05.731000 |  172.31.85.72 |           7273 | 172.31.85.72
                                          Executing single-partition query on tester [ReadStage-1] | 2018-07-05 09:16:05.731000 | 172.31.95.127 |            647 | 172.31.85.72
                                                        Acquiring sstable references [ReadStage-1] | 2018-07-05 09:16:05.731000 | 172.31.95.127 |            716 | 172.31.85.72
                                                           Merging memtable contents [ReadStage-1] | 2018-07-05 09:16:05.731000 | 172.31.95.127 |            765 | 172.31.85.72
                                              Read 1 live rows and 0 tombstone cells [ReadStage-1] | 2018-07-05 09:16:05.731000 | 172.31.95.127 |            980 | 172.31.85.72
                                              Read 1 live rows and 0 tombstone cells [ReadStage-1] | 2018-07-05 09:16:05.731000 | 172.31.95.127 |           1038 | 172.31.85.72
                                                 Enqueuing response to /172.31.85.72 [ReadStage-1] | 2018-07-05 09:16:05.731000 | 172.31.95.127 |           1072 | 172.31.85.72
 Sending REQUEST_RESPONSE message to /172.31.85.72 [MessagingService-Outgoing-/172.31.85.72-Small] | 2018-07-05 09:16:05.731001 | 172.31.95.127 |           1367 | 172.31.85.72
                                          Executing single-partition query on tester [ReadStage-1] | 2018-07-05 09:16:05.732000 | 172.31.82.133 |           1543 | 172.31.85.72
                                  Processing response from /172.31.95.127 [RequestResponseStage-3] | 2018-07-05 09:16:05.732000 |  172.31.85.72 |           7386 | 172.31.85.72
                                                        Acquiring sstable references [ReadStage-1] | 2018-07-05 09:16:05.732000 | 172.31.82.133 |           1612 | 172.31.85.72
                                                           Merging memtable contents [ReadStage-1] | 2018-07-05 09:16:05.733000 | 172.31.82.133 |           1646 | 172.31.85.72
  REQUEST_RESPONSE message received from /172.31.82.133 [MessagingService-Incoming-/172.31.82.133] | 2018-07-05 09:16:05.733000 |  172.31.85.72 |           8446 | 172.31.85.72
                                              Read 1 live rows and 0 tombstone cells [ReadStage-1] | 2018-07-05 09:16:05.733000 | 172.31.82.133 |           1847 | 172.31.85.72
                                  Processing response from /172.31.82.133 [RequestResponseStage-2] | 2018-07-05 09:16:05.733000 |  172.31.85.72 |           8610 | 172.31.85.72
                                              Read 1 live rows and 0 tombstone cells [ReadStage-1] | 2018-07-05 09:16:05.733000 | 172.31.82.133 |           1917 | 172.31.85.72
                                                   Initiating read-repair [RequestResponseStage-2] | 2018-07-05 09:16:05.733000 |  172.31.85.72 |           8677 | 172.31.85.72
                                                 Enqueuing response to /172.31.85.72 [ReadStage-1] | 2018-07-05 09:16:05.733001 | 172.31.82.133 |           1965 | 172.31.85.72
 Sending REQUEST_RESPONSE message to /172.31.85.72 [MessagingService-Outgoing-/172.31.85.72-Small] | 2018-07-05 09:16:05.733001 | 172.31.82.133 |           2213 | 172.31.85.72
                                                                                  Request complete | 2018-07-05 09:16:05.733119 |  172.31.85.72 |           9119 | 172.31.85.72


cqlsh:cycling_alt> CONSISTENCY ALL;
Consistency level set to ALL.
cqlsh:cycling_alt> SELECT * FROM cycling_alt.tester WHERE id = 0;

 id | col1 | col2
----+------+------
  0 |    0 |    0

(1 rows)

Tracing session: 26463470-8034-11e8-8621-0f4156f9c36b

 activity                                                                                          | timestamp                  | source        | source_elapsed | client
---------------------------------------------------------------------------------------------------+----------------------------+---------------+----------------+--------------
                                                                                Execute CQL3 query | 2018-07-05 09:16:50.359000 |  172.31.85.72 |              0 | 172.31.85.72
              Parsing SELECT * FROM cycling_alt.tester WHERE id = 0; [Native-Transport-Requests-1] | 2018-07-05 09:16:50.360000 |  172.31.85.72 |            357 | 172.31.85.72
                                                 Preparing statement [Native-Transport-Requests-1] | 2018-07-05 09:16:50.360000 |  172.31.85.72 |            462 | 172.31.85.72
                                    reading data from /172.31.82.133 [Native-Transport-Requests-1] | 2018-07-05 09:16:50.360000 |  172.31.85.72 |            719 | 172.31.85.72
                                  reading digest from /172.31.95.127 [Native-Transport-Requests-1] | 2018-07-05 09:16:50.360000 |  172.31.85.72 |            813 | 172.31.85.72
                                          Executing single-partition query on tester [ReadStage-2] | 2018-07-05 09:16:50.360000 |  172.31.85.72 |            925 | 172.31.85.72
                                                        Acquiring sstable references [ReadStage-2] | 2018-07-05 09:16:50.360000 |  172.31.85.72 |            968 | 172.31.85.72
                                                           Merging memtable contents [ReadStage-2] | 2018-07-05 09:16:50.360001 |  172.31.85.72 |            998 | 172.31.85.72
                                              Read 1 live rows and 0 tombstone cells [ReadStage-2] | 2018-07-05 09:16:50.360001 |  172.31.85.72 |           1160 | 172.31.85.72
           Sending READ message to /172.31.82.133 [MessagingService-Outgoing-/172.31.82.133-Small] | 2018-07-05 09:16:50.361000 |  172.31.85.72 |           1484 | 172.31.85.72
           Sending READ message to /172.31.95.127 [MessagingService-Outgoing-/172.31.95.127-Small] | 2018-07-05 09:16:50.361000 |  172.31.85.72 |           1638 | 172.31.85.72
                READ message received from /172.31.85.72 [MessagingService-Incoming-/172.31.85.72] | 2018-07-05 09:16:50.362000 | 172.31.82.133 |             15 | 172.31.85.72
                READ message received from /172.31.85.72 [MessagingService-Incoming-/172.31.85.72] | 2018-07-05 09:16:50.362000 | 172.31.95.127 |             14 | 172.31.85.72
                                          Executing single-partition query on tester [ReadStage-1] | 2018-07-05 09:16:50.362000 | 172.31.95.127 |            488 | 172.31.85.72
                                                        Acquiring sstable references [ReadStage-1] | 2018-07-05 09:16:50.362000 | 172.31.95.127 |            542 | 172.31.85.72
                                                           Merging memtable contents [ReadStage-1] | 2018-07-05 09:16:50.362000 | 172.31.95.127 |            581 | 172.31.85.72
                                          Executing single-partition query on tester [ReadStage-2] | 2018-07-05 09:16:50.363000 | 172.31.82.133 |            905 | 172.31.85.72
                                              Read 1 live rows and 0 tombstone cells [ReadStage-1] | 2018-07-05 09:16:50.363000 | 172.31.95.127 |            739 | 172.31.85.72
                                                        Acquiring sstable references [ReadStage-2] | 2018-07-05 09:16:50.363000 | 172.31.82.133 |            967 | 172.31.85.72
                                              Read 1 live rows and 0 tombstone cells [ReadStage-1] | 2018-07-05 09:16:50.363000 | 172.31.95.127 |            827 | 172.31.85.72
                                                           Merging memtable contents [ReadStage-2] | 2018-07-05 09:16:50.363000 | 172.31.82.133 |           1003 | 172.31.85.72
                                                 Enqueuing response to /172.31.85.72 [ReadStage-1] | 2018-07-05 09:16:50.363000 | 172.31.95.127 |            864 | 172.31.85.72
                                              Read 1 live rows and 0 tombstone cells [ReadStage-2] | 2018-07-05 09:16:50.363000 | 172.31.82.133 |           1137 | 172.31.85.72
                                                 Enqueuing response to /172.31.85.72 [ReadStage-2] | 2018-07-05 09:16:50.363000 | 172.31.82.133 |           1173 | 172.31.85.72
  REQUEST_RESPONSE message received from /172.31.95.127 [MessagingService-Incoming-/172.31.95.127] | 2018-07-05 09:16:50.364000 |  172.31.85.72 |           4336 | 172.31.85.72
 Sending REQUEST_RESPONSE message to /172.31.85.72 [MessagingService-Outgoing-/172.31.85.72-Small] | 2018-07-05 09:16:50.364000 | 172.31.95.127 |           1835 | 172.31.85.72
                                  Processing response from /172.31.95.127 [RequestResponseStage-3] | 2018-07-05 09:16:50.364000 |  172.31.85.72 |           4442 | 172.31.85.72
  REQUEST_RESPONSE message received from /172.31.82.133 [MessagingService-Incoming-/172.31.82.133] | 2018-07-05 09:16:50.364000 |  172.31.85.72 |           4713 | 172.31.85.72
                                  Processing response from /172.31.82.133 [RequestResponseStage-3] | 2018-07-05 09:16:50.364000 |  172.31.85.72 |           4853 | 172.31.85.72
 Sending REQUEST_RESPONSE message to /172.31.85.72 [MessagingService-Outgoing-/172.31.85.72-Small] | 2018-07-05 09:16:50.365000 | 172.31.82.133 |           2324 | 172.31.85.72
                                                                                  Request complete | 2018-07-05 09:16:50.364168 |  172.31.85.72 |           5168 | 172.31.85.72


============
DATADOG 
============
# DD_API_KEY=0a7ab0cae3d67c7ab10a918db37020b6 bash -c "$(curl -L https://raw.githubusercontent.com/DataDog/datadog-agent/master/cmd/agent/install_script.sh)"

* Adding your API key to the Agent configuration: /etc/datadog-agent/datadog.yaml

* Starting the Agent...

datadog-agent start/running, process 4608


Your Agent is running and functioning properly. It will continue to run in the
background and submit metrics to Datadog.

If you ever want to stop the Agent, run:

     stop datadog-agent

And to run it again run:

     start datadog-agent

root@ip-172-31-85-72:~# start datadog-agent
start: Job is already running: datadog-agent

# cp /etc/datadog-agent/conf.d/cassandra.d/conf.yaml.example /etc/datadog-agent/conf.d/cassandra.d/conf.yaml

# service datadog-agent restart
datadog-agent stop/waiting
datadog-agent start/running, process 4839

# datadog-agent status | grep cassandra
    cassandra
      instance_name : cassandra-localhost-7199
      message : Number of returned metrics is too high for instance: cassandra-localhost-7199. Please read http://docs.datadoghq.com/integrations/java/ or get in touch with Datadog Support for more details. Truncating to 350 metrics.
      
# wget https://raw.githubusercontent.com/DataDog/Miscellany/master/dashconverter.py
--2018-07-06 09:17:02--  https://raw.githubusercontent.com/DataDog/Miscellany/master/dashconverter.py
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.32.133
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.32.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 8512 (8.3K) [text/plain]
Saving to: ‘dashconverter.py’

100%[================================================================================================================================================>] 8,512       --.-K/s   in 0s      

2018-07-06 09:17:02 (122 MB/s) - ‘dashconverter.py’ saved [8512/8512]

# python dashconverter.py --api-key 0a7ab0cae3d67c7ab10a918db37020b6 --app-key 4a24034bfff813cd5d991871aff3763a857a0ac9 --title "Cassandra Overview Screenboard" 810114
Traceback (most recent call last):
  File "dashconverter.py", line 1, in <module>
    from datadog import initialize, api
ImportError: No module named datadog
