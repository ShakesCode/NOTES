===========================
REDIS CLUSTER 
===========================
https://redis.io/topics/cluster-tutorial  --> on same machine

cheat sheet - https://willwarren.com/2017/10/redis-cluster-cheatsheet/

https://stackoverflow.com/questions/37655523/redis-cluster-different-machines --> on different machines 
--> with redis_trib.rb

https://www.linode.com/docs/applications/big-data/how-to-install-and-configure-a-redis-cluster-on-ubuntu-1604 --> multi mach
--> with redis_trib.rb
--> with sharding

https://www.digitalocean.com/community/tutorials/how-to-configure-a-redis-cluster-on-ubuntu-14-04 --> differenent machines 
--> more detailed 
--> with slaveof directive

https://developers.redhat.com/blog/2017/06/09/how-to-setup-a-redis-server-cluster-on-red-hat/ --> diff mach, with script

https://redislabs.com/redis-enterprise-documentation/administering/cluster-operations/new-cluster-setup/


====================================
INSTALL RUBY AND GEMS
====================================
- INSTALL RUBY
Needed for redis-trib.rb
# apt-get install ruby

- INSTALL REDIS GEM
# gem install redis

====================================
SET UP CONFIG FILES AND START REDIS
====================================

- TOPOLOGY
Machines - m1,m2,m3
Master port on every machine - 6379
Slave port on every machine  - 6380

- CONFIG FILES ON EACH MACHINE
/etc/redis/redis_6379.conf
/etc/redis/redis_6380.conf

- CONTENT OF CONFIG FILES
First, create them same as in non-cluster (follow redis-non-cluster document in this repo)

Add/uncomment the following in them:
cluster-enabled yes
cluster-config-file /etc/redis/nodes-6379.conf
cluster-node-timeout 15000

- INIT.D FILES
Create init.d files for both ports as in non-cluster 
Create rc files using update-rc.d as in non-cluster

====================================
CREATE CLUSTER
====================================

-----------------
START REDIS
-----------------
# Start Redis on each port on each machine
# /etc/init.d/redis_6379 start
# /etc/init.d/redis_6380 start

# ps -ef |grep redis
root      1566     1  0 11:54 ?        00:00:07 /usr/local/bin/redis-server 0.0.0.0:6379 [cluster]
root      1572     1  0 11:54 ?        00:00:06 /usr/local/bin/redis-server 0.0.0.0:6380 [cluster]

-----------------
CREATE CLUSTER
-----------------
# create cluster with three masters
<redis software directory>/src/redis-trib.rb create 110.163.70.47:6379 110.163.71.39:6379 110.163.71.91:6379

-----------------
ADD SLAVES
-----------------
# add one slave each
Syntax:
./redis-trib.rb add-node --slave --master-id <id of the master> <slave ip>:<slave port> <master ip>:<master port>

./redis-trib.rb add-node --slave --master-id 84ff05dd70877ac1dc693bcb97a8b62f59c918ca 110.163.71.39:6380 110.163.70.47:6379
./redis-trib.rb add-node --slave --master-id ddbeecc18ff64f4e4b5376718a2b00564f3e4c04 110.163.71.91:6380 110.163.71.39:6379
./redis-trib.rb add-node --slave --master-id bb98cc4aa5631764f608c8b086285e22a19485a7 110.163.70.47:6380 110.163.71.91:6379

Alternate syntax:
./redis-trib.rb add-node --slave <slave ip>:<slave port> <any random master ip>:<its port>
Note that the command line here is exactly like the one we used to add a new master, 
so we are not specifying to which master we want to add the replica. In this case what happens is that 
redis-trib will add the new node as replica of a random master among the masters with less replicas.

-----------------
VERIFY
-----------------
Set a key - connecting to slave

110.163.71.39:6380> set mykey helloagain
-> Redirected to slot [14687] located at 10.63.71.91:6379
OK
(0.75s)

Set a key - connecting to master
Get the key - connecting to master, slave etc...

-----------------
SCREEN OUTPUTS
-----------------

- CREATE CLUSTER
root@redisc1:/home/redis/redis-4.0.2/src# ./redis-trib.rb create 110.163.70.47:6379 110.163.71.39:6379 110.163.71.91:6379
>>> Creating cluster
>>> Performing hash slots allocation on 3 nodes...
Using 3 masters:
110.163.70.47:6379
110.163.71.39:6379
110.163.71.91:6379
M: 84ff05dd70877ac1dc693bcb97a8b62f59c918ca 110.163.70.47:6379
   slots:0-5460 (5461 slots) master
M: ddbeecc18ff64f4e4b5376718a2b00564f3e4c04 110.163.71.39:6379
   slots:5461-10922 (5462 slots) master
M: bb98cc4aa5631764f608c8b086285e22a19485a7 110.163.71.91:6379
   slots:10923-16383 (5461 slots) master
Can I set the above configuration? (type 'yes' to accept): yes
>>> Nodes configuration updated
>>> Assign a different config epoch to each node
>>> Sending CLUSTER MEET messages to join the cluster
Waiting for the cluster to join..
>>> Performing Cluster Check (using node 110.163.70.47:6379)
M: 84ff05dd70877ac1dc693bcb97a8b62f59c918ca 110.163.70.47:6379
   slots:0-5460 (5461 slots) master
   0 additional replica(s)
M: bb98cc4aa5631764f608c8b086285e22a19485a7 110.163.71.91:6379
   slots:10923-16383 (5461 slots) master
   0 additional replica(s)
M: ddbeecc18ff64f4e4b5376718a2b00564f3e4c04 110.163.71.39:6379
   slots:5461-10922 (5462 slots) master
   0 additional replica(s)
[OK] All nodes agree about slots configuration.
>>> Check for open slots...
>>> Check slots coverage...
[OK] All 16384 slots covered.

- ADDING SLAVE
root@redisc1:/home/redis/redis-4.0.2/src# ./redis-trib.rb add-node --slave --master-id 84ff05dd70877ac1dc693bcb97a8b62f59c918ca 110.163.71.39:6380 110.163.70.47:6379
>>> Adding node 110.163.71.39:6380 to cluster 110.163.70.47:6379
>>> Performing Cluster Check (using node 110.163.70.47:6379)
M: 84ff05dd70877ac1dc693bcb97a8b62f59c918ca 110.163.70.47:6379
   slots:0-5460 (5461 slots) master
   0 additional replica(s)
M: bb98cc4aa5631764f608c8b086285e22a19485a7 110.163.71.91:6379
   slots:10923-16383 (5461 slots) master
   0 additional replica(s)
M: ddbeecc18ff64f4e4b5376718a2b00564f3e4c04 110.163.71.39:6379
   slots:5461-10922 (5462 slots) master
   0 additional replica(s)
[OK] All nodes agree about slots configuration.
>>> Check for open slots...
>>> Check slots coverage...
[OK] All 16384 slots covered.
>>> Send CLUSTER MEET to node 110.163.71.39:6380 to make it join the cluster.
Waiting for the cluster to join.
>>> Configure node as replica of 110.163.70.47:6379.
[OK] New node added correctly.

root@redisc1:/home/redis/redis-4.0.2/src# ./redis-trib.rb add-node --slave --master-id ddbeecc18ff64f4e4b5376718a2b00564f3e4c04 110.163.71.91:6380 110.163.71.39:6379
>>> Adding node 110.163.71.91:6380 to cluster 110.163.71.39:6379
>>> Performing Cluster Check (using node 110.163.71.39:6379)
M: ddbeecc18ff64f4e4b5376718a2b00564f3e4c04 110.163.71.39:6379
   slots:5461-10922 (5462 slots) master
   0 additional replica(s)
M: bb98cc4aa5631764f608c8b086285e22a19485a7 110.163.71.91:6379
   slots:10923-16383 (5461 slots) master
   0 additional replica(s)
S: 790d23dd6021f503ce2ef8a41f6670ba80bf1281 110.163.71.39:6380
   slots: (0 slots) slave
   replicates 84ff05dd70877ac1dc693bcb97a8b62f59c918ca
M: 84ff05dd70877ac1dc693bcb97a8b62f59c918ca 110.163.70.47:6379
   slots:0-5460 (5461 slots) master
   1 additional replica(s)
[OK] All nodes agree about slots configuration.
>>> Check for open slots...
>>> Check slots coverage...
[OK] All 16384 slots covered.
>>> Send CLUSTER MEET to node 110.163.71.91:6380 to make it join the cluster.
Waiting for the cluster to join.
>>> Configure node as replica of 110.163.71.39:6379.
[OK] New node added correctly.


root@redisc1:/home/redis/redis-4.0.2/src# ./redis-trib.rb add-node --slave --master-id bb98cc4aa5631764f608c8b086285e22a19485a7 110.163.70.47:6380 110.163.71.91:6379
>>> Adding node 110.163.70.47:6380 to cluster 110.163.71.91:6379
>>> Performing Cluster Check (using node 110.163.71.91:6379)
M: bb98cc4aa5631764f608c8b086285e22a19485a7 110.163.71.91:6379
   slots:10923-16383 (5461 slots) master
   0 additional replica(s)
M: 84ff05dd70877ac1dc693bcb97a8b62f59c918ca 110.163.70.47:6379
   slots:0-5460 (5461 slots) master
   1 additional replica(s)
S: 790d23dd6021f503ce2ef8a41f6670ba80bf1281 110.163.71.39:6380
   slots: (0 slots) slave
   replicates 84ff05dd70877ac1dc693bcb97a8b62f59c918ca
S: 1dd9ce2b695e9244d749fa068be5e6510e8c8d03 110.163.71.91:6380
   slots: (0 slots) slave
   replicates ddbeecc18ff64f4e4b5376718a2b00564f3e4c04
M: ddbeecc18ff64f4e4b5376718a2b00564f3e4c04 110.163.71.39:6379
   slots:5461-10922 (5462 slots) master
   1 additional replica(s)
[OK] All nodes agree about slots configuration.
>>> Check for open slots...
>>> Check slots coverage...
[OK] All 16384 slots covered.
>>> Send CLUSTER MEET to node 110.163.70.47:6380 to make it join the cluster.
Waiting for the cluster to join.
>>> Configure node as replica of 110.163.71.91:6379.
[OK] New node added correctly.
root@redisc1:/home/redis/redis-4.0.2/src# 

- VERIFY CLUSTER - BEFORE ADDING SLAVES
# redis-cli -c -h 110.163.71.91 -p 6379
110.163.71.91:6379> cluster nodes
ddbeecc18ff64f4e4b5376718a2b00564f3e4c04 110.163.71.39:6379@16379 master - 0 1510574587580 2 connected 5461-10922
84ff05dd70877ac1dc693bcb97a8b62f59c918ca 110.163.70.47:6379@16379 master - 0 1510574586578 1 connected 0-5460
bb98cc4aa5631764f608c8b086285e22a19485a7 192.168.10.14:6379@16379 myself,master - 0 1510574586000 3 connected 10923-16383

- VERIFY CLUSTER - AFTER ADDING SLAVES
root@redisc1:/home/redis/redis-4.0.2/src# redis-cli -c -h 110.163.71.91 -p 6379
110.163.71.91:6379> cluster nodes
84ff05dd70877ac1dc693bcb97a8b62f59c918ca 110.163.70.47:6379@16379 master - 0 1510575166000 1 connected 0-5460
790d23dd6021f503ce2ef8a41f6670ba80bf1281 110.163.71.39:6380@16380 slave 84ff05dd70877ac1dc693bcb97a8b62f59c918ca 0 1510575166801 1 connected
1dd9ce2b695e9244d749fa068be5e6510e8c8d03 110.163.71.91:6380@16380 slave ddbeecc18ff64f4e4b5376718a2b00564f3e4c04 0 1510575166000 2 connected
f818f1c94855f9d37fe3505914bdec74eec3dcb5 110.163.70.47:6380@16380 slave bb98cc4aa5631764f608c8b086285e22a19485a7 0 1510575165798 3 connected
ddbeecc18ff64f4e4b5376718a2b00564f3e4c04 110.163.71.39:6379@16379 master - 0 1510575167803 2 connected 5461-10922
bb98cc4aa5631764f608c8b086285e22a19485a7 192.168.10.14:6379@16379 myself,master - 0 1510575165000 3 connected 10923-16383

- VERIFY AGAIN
110.163.71.91:6380> cluster info
cluster_state:ok
cluster_slots_assigned:16384
cluster_slots_ok:16384
cluster_slots_pfail:0
cluster_slots_fail:0
cluster_known_nodes:6
cluster_size:3
cluster_current_epoch:3
cluster_my_epoch:2
cluster_stats_messages_ping_sent:4037
cluster_stats_messages_pong_sent:4244
cluster_stats_messages_meet_sent:4
cluster_stats_messages_sent:8285
cluster_stats_messages_ping_received:4243
cluster_stats_messages_pong_received:4041
cluster_stats_messages_meet_received:1
cluster_stats_messages_received:8285

----------------------------------
REMOVING MASTER NODES FROM CLUSTER - WITH NO SLAVES UNDER THEM
----------------------------------
# ./redis-trib.rb del-node 10.63.70.47:6380 0d9356adc1fd9291e312d7ee529255074cbda133
>>> Removing node 0d9356adc1fd9291e312d7ee529255074cbda133 from cluster 10.63.70.47:6380
[ERR] Node 10.63.70.47:6380 is not empty! Reshard data away and try again.

# ./redis-trib.rb rebalance --weight 0d9356adc1fd9291e312d7ee529255074cbda133=0 10.63.70.47:6380
>>> Performing Cluster Check (using node 110.63.170.147:6380)
[OK] All nodes agree about slots configuration.
>>> Check for open slots...
>>> Check slots coverage...
[OK] All 16384 slots covered.
>>> Rebalancing across 3 nodes. Total weight = 2.0
Moving 2731 slots from 110.63.170.147:6380 to 110.63.171.191:6380
#############################################################
#############################################################
#############################################################

# ./redis-trib.rb del-node 110.63.170.147:6380 0d9356adc1fd9291e312d7ee529255074cbda133
>>> Removing node 0d9356adc1fd9291e312d7ee529255074cbda133 from cluster 10.63.70.47:6380
>>> Sending CLUSTER FORGET messages to the cluster...
>>> SHUTDOWN the node.

- SIMILARLY, REBALANCE THE OTHER NODE AND DROP IT

- THE LAST NODE CAN BE A PROBLEM
It wont drop when running
It wont drop when stopped
# ./redis-trib.rb del-node 110.63.171.191:6380 1c3d30f6de183480fdfe1266d8a5adaccd04d8df
>>> Removing node 1c3d30f6de183480fdfe1266d8a5adaccd04d8df from cluster 110.63.171.191:6380
[ERR] Sorry, can't connect to node 110.63.171.191:6380

