=============================
NOTES - EXPERT ORACLE EXADATA
=============================

=======================
OFFLOADING - SMART SCAN
=======================
- Offloading
Offload query execution to storage layer.
Storage server part-processes the query and sends already partly-filtered data to DB server.

alter session set cell_offload_processing=false|true;

- Smart Scan
Direct-path-reads is a pre-requisite for Smart Scans - set the session parameter _SERIAL_DIRECT_READ=ALWAYS.
To disable smart-scan set  CELL_OFFLOAD_PROCESSING=FALSE.

Following type of filtering is done by smart scan in storage itself:
  Column projection, predicate filtering, storage indexes
  Simple joins, functions offloading, virtual column evaluation, decryption
Smart-flash later stores that pre-filtered data before sending to memory blocks

Plan shows this as "TABLE ACCESS STORAGE FULL".
V$SQL has some new offload related columns:
   select SQL_ID, IO_CELL_OFFLOAD_ELIGIBLE_BYTES eligible,
     IO_INTERCONNECT_BYTES actual,
     100*(IO_CELL_OFFLOAD_ELIGIBLE_BYTES-IO_INTERCONNECT_BYTES)
     /IO_CELL_OFFLOAD_ELIGIBLE_BYTES "IO_SAVED_%", sql_text
    from v$sql where SQL_ID in ('8m5zmxka24vyk', '0563r8vdy9t2y');

Column projection - return only the columns of interest (the columns in the SELECT and WHERE) - not the whole row



==============================================
HYBRID COLUMNAR COMPRESSION
==============================================
- Basic compression 
Basic compression works only with direct-path operations like 'insert-append'. 
Direct bypasses buffer-cache and stores rows above HWM.
Updates render rows to uncompress and stay so (DML unfriendly).
Makes pctfree 0 upon compression, wasting space.

- OLTP compression
Does not compress immediately - does so upon reaching a threshold.
No need for 'direct' operations.
DML permitted and benefit from compression.

- HCC Hybrid Columnar Compression
Supported on Exadata, ZFS Storage Appliance,Pillar Axiom or Oracle FS1 storage
If stored on other types of storage - smart scans cannot be used, first it has to decompress before reading

Limitations/Cautions:
Insert direct path triggers HCC - else it is columnar format (TBD - what is this 'columnar format')
Updates make rows to use row-format
Updates cause CU-level locking up to 32000 rows

Importing HCC to non-HCC table:
$ impdp ... transform=table_compression_clause:nocompress

-- What is HCC
Traditionally, rows are stored in blocks 
  - efficient for OLT where specific rows are queried and updated.
  - not efficient for column-lookups as query has to then scan through multiple blocks - wider row the worse
In 'columnar' db's blocks store column values than whole rows 
  - efficient for DSS where specific columns of many rows or aggregations are required
  - not efficient for full-row lookups as query has to scan through multiple blocks
Blocks can be usually greater than usual 8k.
Provides still 'table access by index rowid'.
CU - compression unit - contiguous multiple blocks to store columnar data - and the row information to which column data belongs.
Access - block oriented, smart scan

-- Limitations:
Requires direct path - so, use more as part of ILM or static data than as oltp compression alternative
Updates move rows to OLTP compression, out of CU
Non direct operations store rows outside of CU and not as HCC

-- Levels
Query Low 4x, Query High 6x, Archite Low 7x, Archive High 12x

-- Package
DBMS_COMPRESSION

-- Tracing sessions (Universal Tracing Facility - UTS)
SQL> oradebug doc component ADVCMP
SQL> alter session set events 'trace[ADVCMP_MAIN.*] disk=high';
SQL> create table t2_ql
      column store compress for query low
      as select * from t2;
SQL> alter session set events 'trace[ADVCMP_MAIN.*] off';

-- Useful sqls

select table_name,compression,compress_for
  from user_tables;

select s.segment_name, s.bytes/power(1024,2) mb, s.blocks,
  t.compression, t.compress_for, num_rows
  from user_segments s, user_tables t
  where s.segment_name = t.table_name
  and s.segment_name like 'T1%'
  order by mb;
  
select id, rowid,
  dbms_compression.get_compression_type(user, 'T1_QL', rowid) compType
  from t1_ql where rownum < 3;
  
create table t_ql ... column store compress for query low;

alter table t1 modify partition p_jun_2013 column store compress for query high;

To find the type of compression for various rows:  
select decode (dbms_compression.get_compression_type(user,'T1_QH',rowid),
       1,  'COMP_NOCOMPRESS',
       2,  'COMP_FOR_OLTP',  --> in 12c it is COMP_ADVANCED
       4,  'COMP_FOR_QUERY_HIGH',
       8,  'COMP_FOR_QUERY_LOW',
       16, 'COMP_FOR_ARCHIVE_HIGH',
       32, 'COMP_FOR_ARCHIVE_LOW',
       64, 'COMP_BLOCK',  --> COMP_BLOCK is OLTP-64
       'OTHER') type
   from T1
  where rownum < 100;

Blog that has some tracing and such: 
http://kerryosborne.oracle-guy.com/2011/01/ehcc-mechanics-proof-that-whole-cus-are-not-decompressed/
